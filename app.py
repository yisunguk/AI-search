import streamlit as st
import os
import time
import uuid
from datetime import datetime, timedelta
from azure.storage.blob import BlobServiceClient, generate_blob_sas, BlobSasPermissions, generate_container_sas, ContainerSasPermissions
from azure.ai.translation.document import DocumentTranslationClient, DocumentTranslationInput, TranslationTarget
from azure.core.credentials import AzureKeyCredential
import urllib.parse
import requests
import fitz # PyMuPDF for page count

# Search Manager Import
from search_manager import AzureSearchManager

# Chat Manager Import  
from chat_manager_v2 import AzureOpenAIChatManager
from doc_intel_manager import DocumentIntelligenceManager
import excel_manager

# Authentication imports
from utils.auth_manager import AuthManager
from modules.login_page import render_login_page
import extra_streamlit_components as stx

# -----------------------------
# ì„¤ì • ë° ë¹„ë°€ ê´€ë¦¬
# -----------------------------
st.set_page_config(page_title="ì¸í…”ë¦¬ì „íŠ¸ ë‹¤íë¨¼íŠ¸", page_icon="ğŸ—ï¸", layout="centered")

# Custom CSS for larger tab labels
st.markdown("""
<style>
    /* Increase font size for tab labels */
    button[data-baseweb="tab"] {
        font-size: 20px !important;
    }
    button[data-baseweb="tab"] p {
        font-size: 20px !important;
        font-weight: 600 !important;
    }
</style>
""", unsafe_allow_html=True)

def get_secret(key):
    if key in st.secrets:
        return st.secrets[key]
    return os.environ.get(key)

# í•„ìˆ˜ ìê²© ì¦ëª…
# 1. Storage
STORAGE_CONN_STR = get_secret("AZURE_STORAGE_CONNECTION_STRING")
CONTAINER_NAME = get_secret("AZURE_BLOB_CONTAINER_NAME") or "blob-leesunguk"

# 2. Translator
TRANSLATOR_KEY = get_secret("AZURE_TRANSLATOR_KEY")
TRANSLATOR_ENDPOINT = get_secret("AZURE_TRANSLATOR_ENDPOINT")

# 3. Search
SEARCH_ENDPOINT = get_secret("AZURE_SEARCH_ENDPOINT")
SEARCH_KEY = get_secret("AZURE_SEARCH_KEY")
SEARCH_INDEX_NAME = get_secret("AZURE_SEARCH_INDEX_NAME") or "pdf-search-index"
SEARCH_INDEXER_NAME = "pdf-indexer"
SEARCH_DATASOURCE_NAME = "blob-datasource"

# 4. Azure OpenAI
AZURE_OPENAI_ENDPOINT = get_secret("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_KEY = get_secret("AZURE_OPENAI_KEY")
AZURE_OPENAI_DEPLOYMENT = get_secret("AZURE_OPENAI_DEPLOYMENT") or get_secret("AZURE_OPENAI_DEPLOYMENT_NAME")
AZURE_OPENAI_API_VERSION = get_secret("AZURE_OPENAI_API_VERSION")

# 5. Document Intelligence
AZURE_DOC_INTEL_ENDPOINT = get_secret("AZURE_DOC_INTEL_ENDPOINT")
AZURE_DOC_INTEL_KEY = get_secret("AZURE_DOC_INTEL_KEY")

# -----------------------------
# Azure í´ë¼ì´ì–¸íŠ¸ í—¬í¼
# -----------------------------
def get_blob_service_client():
    if not STORAGE_CONN_STR:
        st.error("Azure Storage Connection Stringì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.stop()
    return BlobServiceClient.from_connection_string(STORAGE_CONN_STR)

def get_translation_client():
    if not TRANSLATOR_KEY or not TRANSLATOR_ENDPOINT:
        st.error("Azure Translator Key ë˜ëŠ” Endpointê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.stop()
    return DocumentTranslationClient(TRANSLATOR_ENDPOINT, AzureKeyCredential(TRANSLATOR_KEY))

def get_search_manager():
    if not SEARCH_ENDPOINT or not SEARCH_KEY:
        st.error("Azure Search Endpoint ë˜ëŠ” Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.stop()
    return AzureSearchManager(SEARCH_ENDPOINT, SEARCH_KEY, SEARCH_INDEX_NAME)

def get_chat_manager():
    if not AZURE_OPENAI_ENDPOINT or not AZURE_OPENAI_KEY:
        st.error("Azure OpenAI Endpoint ë˜ëŠ” Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.stop()
    
    # Get search manager for Client-Side RAG
    search_manager = get_search_manager()
    
    return AzureOpenAIChatManager(
        endpoint=AZURE_OPENAI_ENDPOINT,
        api_key=AZURE_OPENAI_KEY,
        deployment_name=AZURE_OPENAI_DEPLOYMENT,
        api_version=AZURE_OPENAI_API_VERSION,
        search_manager=search_manager,
        storage_connection_string=STORAGE_CONN_STR,
        container_name=CONTAINER_NAME
    )

def get_doc_intel_manager():
    if not AZURE_DOC_INTEL_ENDPOINT or not AZURE_DOC_INTEL_KEY:
        st.error("Azure Document Intelligence Endpoint ë˜ëŠ” Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.stop()
    return DocumentIntelligenceManager(AZURE_DOC_INTEL_ENDPOINT, AZURE_DOC_INTEL_KEY)

def generate_sas_url(blob_service_client, container_name, blob_name=None, permission="r", expiry_hours=1, content_disposition=None):
    """
    Blob ë˜ëŠ” Containerì— ëŒ€í•œ SAS URL ìƒì„±
    blob_nameì´ ìˆìœ¼ë©´ Blob SAS, ì—†ìœ¼ë©´ Container SAS (Writeìš©)
    """
    import urllib.parse
    
    account_name = blob_service_client.account_name
    
    # Connection Stringìœ¼ë¡œ ìƒì„±ëœ ê²½ìš° credentialì€ dictì¼ ìˆ˜ ìˆìŒ
    if hasattr(blob_service_client.credential, 'account_key'):
        account_key = blob_service_client.credential.account_key
    else:
        account_key = blob_service_client.credential['account_key']
    
    # ì‹œê³„ ì˜¤ì°¨(Clock Skew) ë°©ì§€ë¥¼ ìœ„í•´ ì‹œì‘ ì‹œê°„ì„ 15ë¶„ ì „ìœ¼ë¡œ ì„¤ì •
    start = datetime.utcnow() - timedelta(minutes=15)
    expiry = datetime.utcnow() + timedelta(hours=expiry_hours)
    
    if blob_name:
        # Blob SAS ì‚¬ìš© (íŒŒì¼ ì§ì ‘ ì—´ê¸° ì§€ì›ì„ ìœ„í•´ content_disposition ì„¤ì •)
        import mimetypes
        content_type, _ = mimetypes.guess_type(blob_name)
        if not content_type:
            content_type = "application/octet-stream"
            
        # PDFëŠ” inline, ë‚˜ë¨¸ì§€ëŠ” attachment (ë˜ëŠ” ë¸Œë¼ìš°ì € ê¸°ë³¸ ë™ì‘)
        # ì—‘ì…€ ë“±ì€ ë¸Œë¼ìš°ì €ê°€ ì•Œì•„ì„œ ë‹¤ìš´ë¡œë“œ ì²˜ë¦¬í•¨
        if content_disposition is None:
            content_disposition = "inline" if content_type == "application/pdf" else "attachment"

        sas_token = generate_blob_sas(
            account_name=account_name,
            container_name=container_name,
            blob_name=blob_name,
            account_key=account_key,
            permission=BlobSasPermissions(read=True),
            start=start,
            expiry=expiry,
            content_disposition=content_disposition, 
            content_type=content_type
        )
        
        base_url = f"https://{account_name}.blob.core.windows.net/{container_name}"
        encoded_blob_name = urllib.parse.quote(blob_name, safe='/')
        return f"{base_url}/{encoded_blob_name}?{sas_token}"
        
    else:
        # Container SAS ì‚¬ìš© (í´ë” ì‘ì—…ìš©)
        sas_token = generate_container_sas(
            account_name=account_name,
            container_name=container_name,
            account_key=account_key,
            permission=ContainerSasPermissions(write=True, list=True, read=True, delete=True),
            start=start,
            expiry=expiry
        )
        
        base_url = f"https://{account_name}.blob.core.windows.net/{container_name}"
        return f"{base_url}?{sas_token}"

# -----------------------------
# UI êµ¬ì„±
# -----------------------------


# ì§€ì› ì–¸ì–´ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (API)
@st.cache_data
def get_supported_languages():
    try:
        url = "https://api.cognitive.microsofttranslator.com/languages?api-version=3.0&scope=translation"
        # Accept-Language í—¤ë”ë¥¼ 'ko'ë¡œ ì„¤ì •í•˜ì—¬ ì–¸ì–´ ì´ë¦„ì„ í•œêµ­ì–´ë¡œ ë°›ìŒ
        headers = {"Accept-Language": "ko"}
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        data = response.json()
        
        languages = {}
        for code, info in data['translation'].items():
            # "í•œêµ­ì–´ ì´ë¦„ (ì›ì–´ ì´ë¦„)" í˜•ì‹ìœ¼ë¡œ í‘œì‹œ (ì˜ˆ: ì˜ì–´ (English))
            label = f"{info['name']} ({info['nativeName']})"
            languages[label] = code
        return languages
    except Exception as e:
        st.error(f"ì–¸ì–´ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
        # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì–¸ì–´ ì œê³µ
        return {"í•œêµ­ì–´ (Korean)": "ko", "ì˜ì–´ (English)": "en"}

LANGUAGES = get_supported_languages()

# ì–¸ì–´ ì½”ë“œë³„ íŒŒì¼ ì ‘ë¯¸ì‚¬ ë§¤í•‘ (ê¸°ë³¸ì ìœ¼ë¡œ ëŒ€ë¬¸ì ì½”ë“œë¥¼ ì‚¬ìš©í•˜ë˜, ì¼ë¶€ ì»¤ìŠ¤í…€ ê°€ëŠ¥)
# ì—¬ê¸°ì„œëŠ” ìë™ ìƒì„± ë¡œì§ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ë³„ë„ ë”•ì…”ë„ˆë¦¬ ë¶ˆí•„ìš”, 
# ë‹¤ë§Œ ì¤‘êµ­ì–´ ë“± íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ë¥¼ ìœ„í•´ ë‚¨ê²¨ë‘˜ ìˆ˜ ìˆìŒ.
LANG_SUFFIX_OVERRIDE = {
    "zh-Hans": "CN",
    "zh-Hant": "TW",
}

# Initialize session state for page navigation
if "page" not in st.session_state:
    st.session_state.page = "í™ˆ"

def change_page(page_name):
    st.session_state.page = page_name

# Initialize AuthManager
auth_manager = AuthManager(STORAGE_CONN_STR)

# Initialize Cookie Manager
# Initialize Cookie Manager
cookie_manager = stx.CookieManager(key="auth_cookie_manager")

# Initialize login state
if 'is_logged_in' not in st.session_state:
    st.session_state.is_logged_in = False

# Check for existing session cookie (Auto-login)
if not st.session_state.is_logged_in and not st.session_state.get('just_logged_out', False):
    try:
        auth_email = cookie_manager.get(cookie="auth_email")
        if auth_email:
            # Validate email exists in auth_manager
            user = auth_manager.get_user_by_email(auth_email)
            if user:
                st.session_state.is_logged_in = True
                st.session_state.user_info = user
                st.toast(f"ìë™ ë¡œê·¸ì¸ë˜ì—ˆìŠµë‹ˆë‹¤: {user.get('name')}")
    except Exception as e:
        print(f"Cookie check failed: {e}")

# Check if user is logged in
if not st.session_state.is_logged_in:
    render_login_page(auth_manager, cookie_manager)
    st.stop()

# User is logged in - get their info
user_info = st.session_state.get('user_info', {})
user_role = user_info.get('role', 'guest')
user_perms = user_info.get('permissions', [])

def get_user_folder_name(user_info):
    """Get sanitized user folder name"""
    if not user_info:
        return "guest"
    # Use name but fallback to ID if empty
    name = user_info.get('name', user_info.get('id', 'guest'))
    return name.strip()

user_folder = get_user_folder_name(user_info)

# Define role-based menu permissions (Fallback / Admin)
ALL_MENUS = ["í™ˆ", "ë²ˆì—­í•˜ê¸°", "íŒŒì¼ ë³´ê´€í•¨", "ê²€ìƒ‰ & AI ì±„íŒ…", "ë„ë©´/ìŠ¤í™ ë¹„êµ", "ì—‘ì…€ë°ì´í„° ìë™ì¶”ì¶œ", "ì‚¬ì§„ëŒ€ì§€ ìë™ì‘ì„±", "ì‘ì—…ê³„íš ë° íˆ¬ì…ë¹„ ìë™ì‘ì„±", "ê´€ë¦¬ì ì„¤ì •", "ì‚¬ìš©ì ì„¤ì •", "ë””ë²„ê·¸ (Debug)"]
GUEST_MENUS = ["í™ˆ", "ì‚¬ìš©ì ì„¤ì •"]

if user_role == 'admin':
    available_menus = ALL_MENUS
else:
    # Use assigned permissions, ensuring mandatory menus are present
    available_menus = user_perms if user_perms else GUEST_MENUS
    
    # Map old menu names to new names (Migration fix)
    available_menus = [
        "ë„ë©´/ìŠ¤í™ ë¹„êµ" if menu == "ë„ë©´/ìŠ¤í™ ë¶„ì„" else menu 
        for menu in available_menus
    ]
    # Ensure "í™ˆ" and "ì‚¬ìš©ì ì„¤ì •" are always available
    if "í™ˆ" not in available_menus:
        available_menus.insert(0, "í™ˆ")
    if "ì‚¬ìš©ì ì„¤ì •" not in available_menus:
        available_menus.append("ì‚¬ìš©ì ì„¤ì •")
    
    # Remove "ê´€ë¦¬ì ì„¤ì •" if somehow present for non-admins
    if "ê´€ë¦¬ì ì„¤ì •" in available_menus:
        available_menus.remove("ê´€ë¦¬ì ì„¤ì •")

with st.sidebar:
    # User profile
    st.markdown(f"### ğŸ‘¤ {user_info.get('name', 'User')}")
    st.caption(f"**{user_info.get('email', '')}**")
    st.caption(f"ê¶Œí•œ: {user_role.upper()}")
    
    if st.button("ğŸšª ë¡œê·¸ì•„ì›ƒ", key="logout_btn", use_container_width=True):
        st.session_state.is_logged_in = False
        st.session_state.user_info = None
        st.session_state.just_logged_out = True # Prevent immediate auto-login
        # Delete cookie
        cookie_manager.delete("auth_email")
        st.rerun()
    
    st.divider()
    
    st.header("ë©”ë‰´")
    # Filter menu based on user role
    menu = st.radio("ì´ë™", available_menus, key="page")
    
    st.divider()
    
    if menu == "ë²ˆì—­í•˜ê¸°":
        st.header("ì„¤ì •")
        # í•œêµ­ì–´ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì°¾ê¸°
        default_index = 0
        lang_labels = list(LANGUAGES.keys())
        for i, label in enumerate(lang_labels):
            if "Korean" in label or "í•œêµ­ì–´" in label:
                default_index = i
                break
                
        target_lang_label = st.selectbox("ëª©í‘œ ì–¸ì–´ ì„ íƒ", lang_labels, index=default_index)
        target_lang_code = LANGUAGES[target_lang_label]
        st.info(f"ì„ íƒëœ ëª©í‘œ ì–¸ì–´: {target_lang_code}")

    # ìê²© ì¦ëª… ìƒíƒœ í™•ì¸
    if STORAGE_CONN_STR and TRANSLATOR_KEY and SEARCH_KEY:
        st.success("âœ… Azure ìê²© ì¦ëª… í™•ì¸ë¨")
    else:
        st.warning("âš ï¸ ì¼ë¶€ Azure ìê²© ì¦ëª…ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")

# Common Header for non-Home pages
if menu != "í™ˆ":
    st.title(menu)


if menu == "í™ˆ":
    # Use the new home_chat module with function calling support
    from home_chat import render_home_chat
    chat_manager = get_chat_manager()
    render_home_chat(chat_manager)
    
if menu == "ë²ˆì—­í•˜ê¸°":
    if "translate_uploader_key" not in st.session_state:
        st.session_state.translate_uploader_key = 0

    uploaded_file = st.file_uploader("ë²ˆì—­í•  ë¬¸ì„œ ì—…ë¡œë“œ (PPTX, PDF, DOCX, XLSX ë“±)", type=["pptx", "pdf", "docx", "xlsx"], key=f"translate_{st.session_state.translate_uploader_key}")

    # ì´ì „ ë²ˆì—­ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í‘œì‹œ
    if "last_translation_result" in st.session_state:
        result = st.session_state.last_translation_result
        st.success("âœ… ë²ˆì—­ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        st.markdown(f"[{result['file_name']} ë‹¤ìš´ë¡œë“œ]({result['url']})", unsafe_allow_html=True)
        
        # ê²°ê³¼ë¥¼ ì§€ìš°ê³  ì‹¶ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹«ê¸° ë²„íŠ¼ ì œê³µ (ì„ íƒ ì‚¬í•­)
        if st.button("ê²°ê³¼ ë‹«ê¸°"):
            del st.session_state.last_translation_result
            st.rerun()

    if st.button("ë²ˆì—­ ì‹œì‘", type="primary", disabled=not uploaded_file):
        if not uploaded_file:
            st.error("íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        else:
            with st.spinner("Azure Blobì— íŒŒì¼ ì—…ë¡œë“œ ì¤‘..."):
                try:
                    blob_service_client = get_blob_service_client()
                    container_client = blob_service_client.get_container_client(CONTAINER_NAME)
                    
                    # ì»¨í…Œì´ë„ˆ ì ‘ê·¼ ê¶Œí•œ í™•ì¸
                    try:
                        if not container_client.exists():
                            container_client.create_container()
                    except Exception as e:
                        if "AuthenticationFailed" in str(e):
                            st.error("ğŸš¨ ì¸ì¦ ì‹¤íŒ¨: Azure Storage Keyê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. Secrets ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                            st.stop()
                        else:
                            raise e

                    # íŒŒì¼ëª… ìœ ë‹ˆí¬í•˜ê²Œ ì²˜ë¦¬ (UUID ì œê±°, ë®ì–´ì“°ê¸° í—ˆìš©)
                    # file_uuid = str(uuid.uuid4())[:8] 
                    original_filename = uploaded_file.name
                    input_blob_name = f"{user_folder}/original/{original_filename}"
                    
                    # ì—…ë¡œë“œ
                    blob_client = container_client.get_blob_client(input_blob_name)
                    blob_client.upload_blob(uploaded_file, overwrite=True)
                    
                    st.success("ì—…ë¡œë“œ ì™„ë£Œ! ë²ˆì—­ ìš”ì²­ ì¤‘...")
                    
                    # SAS ìƒì„±
                    source_url = generate_sas_url(blob_service_client, CONTAINER_NAME, input_blob_name)
                    
                    # Target URL ì„¤ì •
                    target_base_url = f"https://{blob_service_client.account_name}.blob.core.windows.net/{CONTAINER_NAME}"
                    # Target URLì€ ì»¨í…Œì´ë„ˆ ë˜ëŠ” í´ë” ê²½ë¡œì—¬ì•¼ í•¨ (íŒŒì¼ ê²½ë¡œ ë¶ˆê°€)
                    # ì‚¬ìš©ìë³„ translated í´ë”ë¡œ ì„¤ì •
                    # URL ì¸ì½”ë”© í•„ìš”
                    encoded_user_folder = urllib.parse.quote(user_folder)
                    target_output_url = f"{target_base_url}/{encoded_user_folder}/translated/?{generate_sas_url(blob_service_client, CONTAINER_NAME).split('?')[1]}"
                    
                except Exception as e:
                    st.error(f"ì—…ë¡œë“œ/SAS ìƒì„± ì‹¤íŒ¨: {e}")
                    st.stop()

            with st.spinner("ë²ˆì—­ ì‘ì—… ìš”ì²­ ë° ëŒ€ê¸° ì¤‘..."):
                try:
                    client = get_translation_client()
                    
                    poller = client.begin_translation(
                        inputs=[
                            DocumentTranslationInput(
                                source_url=source_url,
                                storage_type="File",
                                targets=[
                                    TranslationTarget(
                                        target_url=target_output_url,
                                        language=target_lang_code
                                    )
                                ]
                            )
                        ]
                    )
                    
                    result = poller.result()
                    
                    for doc in result:
                        if doc.status == "Succeeded":
                            st.success(f"ë²ˆì—­ ì™„ë£Œ! (ìƒíƒœ: {doc.status})")
                        else:
                            st.error(f"ë¬¸ì„œ ë²ˆì—­ ì‹¤íŒ¨! (ìƒíƒœ: {doc.status})")
                            if doc.error:
                                st.error(f"ì—ëŸ¬ ì½”ë“œ: {doc.error.code}, ë©”ì‹œì§€: {doc.error.message}")
                    
                    # ê²°ê³¼ íŒŒì¼ ì°¾ê¸°
                    time.sleep(2)
                    # UUID í´ë”ê°€ ì—†ìœ¼ë¯€ë¡œ translated í´ë” ì „ì²´ì—ì„œ í•´ë‹¹ íŒŒì¼ëª… ê²€ìƒ‰
                    output_prefix_search = f"{user_folder}/translated/"
                    output_blobs = list(container_client.list_blobs(name_starts_with=output_prefix_search))
                    
                    # ë°©ê¸ˆ ë²ˆì—­ëœ íŒŒì¼ ì°¾ê¸° (íŒŒì¼ëª… ë§¤ì¹­)
                    # Azure ë²ˆì—­ì€ ì›ë³¸ íŒŒì¼ëª…ì„ ìœ ì§€í•˜ê±°ë‚˜ ì–¸ì–´ ì½”ë“œë¥¼ ë¶™ì„
                    target_blobs = []
                    for blob in output_blobs:
                        if original_filename in blob.name:
                            target_blobs.append(blob)
                    
                    if not target_blobs:
                        st.warning(f"ê²°ê³¼ íŒŒì¼ì„ ì°¾ëŠ” ì¤‘ì…ë‹ˆë‹¤... (ê²½ë¡œ: {output_prefix_search})")
                        # Fallback: list all to debug
                        # all_output = list(container_client.list_blobs(name_starts_with=output_prefix_search))
                        # debug_msg = "\n".join([b.name for b in all_output[:10]])
                        # st.error(f"ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\ní˜„ì¬ í´ë” íŒŒì¼ ëª©ë¡:\n{debug_msg}")
                    else:
                        st.subheader("ë‹¤ìš´ë¡œë“œ")
                        for blob in target_blobs:
                            blob_name = blob.name
                            file_name = blob_name.split("/")[-1]
                            
                            # íŒŒì¼ëª…ì— ì–¸ì–´ ì ‘ë¯¸ì‚¬ ì¶”ê°€ (Rename)
                            suffix = LANG_SUFFIX_OVERRIDE.get(target_lang_code, target_lang_code.upper())
                            name_part, ext_part = os.path.splitext(file_name)
                            
                            # ì´ë¯¸ ì ‘ë¯¸ì‚¬ê°€ ìˆëŠ”ì§€ í™•ì¸ (í˜¹ì‹œ ëª¨ë¥¼ ì¤‘ë³µ ë°©ì§€)
                            if not name_part.endswith(f"_{suffix}"):
                                new_file_name = f"{name_part}_{suffix}{ext_part}"
                                new_blob_name = f"{user_folder}/translated/{new_file_name}"
                                
                                try:
                                    # Rename: Copy to new name -> Delete old
                                    source_blob = container_client.get_blob_client(blob_name)
                                    dest_blob = container_client.get_blob_client(new_blob_name)
                                    
                                    source_sas = generate_sas_url(blob_service_client, CONTAINER_NAME, blob_name)
                                    dest_blob.start_copy_from_url(source_sas)
                                    
                                    # Wait for copy
                                    for _ in range(10):
                                        props = dest_blob.get_blob_properties()
                                        if props.copy.status == "success":
                                            break
                                        time.sleep(0.2)
                                        
                                    source_blob.delete_blob()
                                    
                                    # Update variables for download link
                                    blob_name = new_blob_name
                                    file_name = new_file_name
                                    st.toast(f"íŒŒì¼ëª… ë³€ê²½ë¨: {file_name}")
                                    
                                except Exception as e:
                                    st.warning(f"íŒŒì¼ëª… ë³€ê²½ ì‹¤íŒ¨ (ê¸°ë³¸ ì´ë¦„ìœ¼ë¡œ ìœ ì§€): {e}")

                            # PPTX í°íŠ¸ ë³€ê²½ (Times New Roman)
                            if file_name.lower().endswith(".pptx"):
                                try:
                                    from pptx import Presentation
                                    
                                    # ì„ì‹œ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ
                                    temp_pptx = f"temp_{original_filename}"
                                    blob_client_temp = container_client.get_blob_client(blob_name)
                                    with open(temp_pptx, "wb") as f:
                                        data = blob_client_temp.download_blob().readall()
                                        f.write(data)
                                    
                                    # í°íŠ¸ ë³€ê²½ ë¡œì§
                                    prs = Presentation(temp_pptx)
                                    font_name = "Times New Roman"
                                    
                                    def change_font(shapes):
                                        for shape in shapes:
                                            if shape.has_text_frame:
                                                for paragraph in shape.text_frame.paragraphs:
                                                    for run in paragraph.runs:
                                                        run.font.name = font_name
                                            
                                            if shape.has_table:
                                                for row in shape.table.rows:
                                                    for cell in row.cells:
                                                        if cell.text_frame:
                                                            for paragraph in cell.text_frame.paragraphs:
                                                                for run in paragraph.runs:
                                                                    run.font.name = font_name
                                            
                                            if shape.shape_type == 6: # Group
                                                change_font(shape.shapes)

                                    for slide in prs.slides:
                                        change_font(slide.shapes)
                                    
                                    prs.save(temp_pptx)
                                    
                                    # ë‹¤ì‹œ ì—…ë¡œë“œ (ë®ì–´ì“°ê¸°)
                                    with open(temp_pptx, "rb") as f:
                                        blob_client_temp.upload_blob(f, overwrite=True)
                                    
                                    os.remove(temp_pptx)
                                    st.toast("PPTX í°íŠ¸ ë³€ê²½ ì™„ë£Œ (Times New Roman)")
                                    
                                except Exception as e:
                                    st.warning(f"PPTX í°íŠ¸ ë³€ê²½ ì‹¤íŒ¨: {e}")

                            download_sas = generate_sas_url(blob_service_client, CONTAINER_NAME, blob_name)
                            st.markdown(f"[{file_name} ë‹¤ìš´ë¡œë“œ]({download_sas})", unsafe_allow_html=True)
                            
                            # ê²°ê³¼ ì„¸ì…˜ì— ì €ì¥
                            st.session_state.last_translation_result = {
                                "file_name": file_name,
                                "url": download_sas
                            }
                            
                    # ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ë©´ ì—…ë¡œë” ì´ˆê¸°í™” (í‚¤ ë³€ê²½)
                    st.session_state.translate_uploader_key += 1
                    time.sleep(1) # ì ì‹œ ëŒ€ê¸°
                    st.rerun()
                            
                except Exception as e:
                    st.error(f"ë²ˆì—­ ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

elif menu == "íŒŒì¼ ë³´ê´€í•¨":
    # st.subheader("ğŸ“‚ í´ë¼ìš°ë“œ íŒŒì¼ ë³´ê´€í•¨") - Removed to avoid duplication
    
    st.divider()
    
    if st.button("ğŸ”„ ëª©ë¡ ìƒˆë¡œê³ ì¹¨"):
        st.rerun()
        
    try:
        blob_service_client = get_blob_service_client()
        container_client = blob_service_client.get_container_client(CONTAINER_NAME)
        
        # íƒ­ìœ¼ë¡œ Input/Output êµ¬ë¶„
        tab1, tab2 = st.tabs(["ì›ë³¸ ë¬¸ì„œ (Input)", "ë²ˆì—­ëœ ë¬¸ì„œ (Output)"])
        
        def render_file_list(prefixes, tab_name):
            all_blobs = []
            for prefix in prefixes:
                blobs = list(container_client.list_blobs(name_starts_with=prefix))
                all_blobs.extend(blobs)
            
            # ì¤‘ë³µ ì œê±° (í˜¹ì‹œ ëª¨ë¥¼ ê²½ìš° ëŒ€ë¹„)
            unique_blobs = {b.name: b for b in all_blobs}.values()
            blobs = list(unique_blobs)
            blobs.sort(key=lambda x: x.creation_time, reverse=True)
            
            if not blobs:
                st.info(f"{tab_name}ì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return

            for i, blob in enumerate(blobs):
                file_name = blob.name.split("/")[-1]
                creation_time = blob.creation_time.strftime('%Y-%m-%d %H:%M')
                
                # í´ë” ê²½ë¡œ í‘œì‹œ (ê´€ë¦¬ì í¸ì˜)
                folder_path = "/".join(blob.name.split("/")[:-1])
                
                with st.container():
                    col1, col2, col3 = st.columns([6, 2, 2])
                    
                    with col1:
                        sas_url = generate_sas_url(blob_service_client, CONTAINER_NAME, blob.name)
                        st.markdown(f"**[{file_name}]({sas_url})**")
                        st.caption(f"ğŸ“‚ {folder_path} | ğŸ“… {creation_time} | ğŸ“¦ {blob.size / 1024:.1f} KB")
                    
                    with col2:
                        # ìˆ˜ì • (ì´ë¦„ ë³€ê²½)
                        with st.popover("ìˆ˜ì •"):
                            new_name = st.text_input("ìƒˆ íŒŒì¼ëª…", value=file_name, key=f"rename_{i}_{blob.name}")
                            if st.button("ì´ë¦„ ë³€ê²½", key=f"btn_rename_{i}_{blob.name}"):
                                try:
                                    # ìƒˆ ê²½ë¡œ ìƒì„± (ê¸°ì¡´ í´ë” êµ¬ì¡° ìœ ì§€)
                                    path_parts = blob.name.split("/")
                                    folder = "/".join(path_parts[:-1])
                                    new_blob_name = f"{folder}/{new_name}"
                                    
                                    # ë³µì‚¬ (Renameì€ Copy + Delete)
                                    source_blob = container_client.get_blob_client(blob.name)
                                    dest_blob = container_client.get_blob_client(new_blob_name)
                                    
                                    # SAS URL for Copy Source
                                    source_sas = generate_sas_url(blob_service_client, CONTAINER_NAME, blob.name)
                                    
                                    dest_blob.start_copy_from_url(source_sas)
                                    
                                    # ë³µì‚¬ ì™„ë£Œ ëŒ€ê¸° (ê°„ë‹¨í•œ í´ë§)
                                    for _ in range(10):
                                        props = dest_blob.get_blob_properties()
                                        if props.copy.status == "success":
                                            break
                                        time.sleep(0.5)
                                    
                                    # ì›ë³¸ ì‚­ì œ
                                    source_blob.delete_blob()
                                    st.success("ì´ë¦„ ë³€ê²½ ì™„ë£Œ!")
                                    time.sleep(1)
                                    st.rerun()
                                except Exception as e:
                                    st.error(f"ì´ë¦„ ë³€ê²½ ì‹¤íŒ¨: {e}")

                    with col3:
                        # ì‚­ì œ
                        if st.button("ì‚­ì œ", key=f"del_{prefix}_{i}", type="secondary"):
                            try:
                                container_client.delete_blob(blob.name)
                                st.success("ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                                time.sleep(1)
                                st.rerun()
                            except Exception as e:
                                st.error(f"ì‚­ì œ ì‹¤íŒ¨: {e}")
                    
                    st.divider()

        with tab1:
            input_prefixes = [f"{user_folder}/documents/"]
            if user_role == 'admin':
                input_prefixes.extend(["input/", "gulflng/"])
            render_file_list(input_prefixes, "ë‚´ ë¬¸ì„œ (Documents)")
            
        with tab2:
            output_prefixes = [f"{user_folder}/translated/"]
            if user_role == 'admin':
                output_prefixes.extend(["output/"])
            render_file_list(output_prefixes, "ë²ˆì—­ëœ ë¬¸ì„œ")
                
    except Exception as e:
        st.error(f"íŒŒì¼ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

elif menu == "ê²€ìƒ‰ & AI ì±„íŒ…":
    # Tabs for Search and Chat to preserve state
    tab1, tab2 = st.tabs(["ğŸ” ë¬¸ì„œ ê²€ìƒ‰", "ğŸ¤– AI ì±„íŒ…"])
    
    with tab1:

        st.subheader("ğŸ” PDF ë¬¸ì„œ ê²€ìƒ‰")
        
        # File Uploader for Document Search
        with st.expander("ğŸ“¤ ë¬¸ì„œ ì—…ë¡œë“œ (ë‚´ ë¬¸ì„œ)", expanded=False):
            doc_upload = st.file_uploader("ê²€ìƒ‰í•  ë¬¸ì„œ ì—…ë¡œë“œ", type=['pdf', 'docx', 'txt', 'pptx'], key="doc_search_upload")
            if doc_upload and st.button("ì—…ë¡œë“œ", key="btn_doc_upload"):
                try:
                    blob_service_client = get_blob_service_client()
                    container_client = blob_service_client.get_container_client(CONTAINER_NAME)
                    
                    # file_uuid = str(uuid.uuid4())[:8]
                    # Upload to {user_folder}/documents/ (Flat structure)
                    blob_name = f"{user_folder}/documents/{doc_upload.name}"
                    blob_client = container_client.get_blob_client(blob_name)
                    blob_client.upload_blob(doc_upload, overwrite=True)
                    st.success(f"'{doc_upload.name}' ì—…ë¡œë“œ ì™„ë£Œ! (ì¸ë±ì‹±ì— ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
                except Exception as e:
                    st.error(f"ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # Search Input
        query = st.text_input("ê²€ìƒ‰ì–´ ì…ë ¥", placeholder="ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”...")
        
        # Search Options (Expander)
        with st.expander("âš™ï¸ ê²€ìƒ‰ ì˜µì…˜ ì„¤ì •", expanded=False):
            c1, c2 = st.columns(2)
            with c1:
                use_semantic = st.checkbox("ì‹œë§¨í‹± ë­ì»¤", value=False, help="ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ (Standard Tier ì´ìƒ)")
            with c2:
                search_mode_opt = st.radio("ê²€ìƒ‰ ëª¨ë“œ", ["all (AND)", "any (OR)"], index=0, horizontal=True, help="all: ëª¨ë“  ë‹¨ì–´ í¬í•¨, any: í•˜ë‚˜ë¼ë„ í¬í•¨")
                search_mode = "all" if "all" in search_mode_opt else "any"
        
        
        if query:
            with st.spinner("ê²€ìƒ‰ ì¤‘..."):
                search_manager = get_search_manager()
                
                # Filter by user folder
                # Construct prefix URL: https://{account}.blob.core.windows.net/{container}/{user_folder}/
                account_name = get_blob_service_client().account_name
                # Need to handle spaces in user_folder for URL
                encoded_user_folder = urllib.parse.quote(user_folder)
                prefix_url = f"https://{account_name}.blob.core.windows.net/{CONTAINER_NAME}/{encoded_user_folder}/"
                
                # OData filter: startswith(metadata_storage_path, 'prefix_url')
                # Also allow 'all' access for admin if needed, but user requested isolation.
                # Search Filter Logic
                if user_role == 'admin':
                    # Admin can search everything
                    filter_expr = None
                else:
                    # Regular users are restricted to their folder
                    filter_expr = f"startswith(metadata_storage_path, '{prefix_url}')"
                
                results = search_manager.search(query, filter_expr=filter_expr, use_semantic_ranker=use_semantic, search_mode=search_mode)
                
                if not results:
                    st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.success(f"ì´ {len(results)}ê°œì˜ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                    for result in results:
                        with st.container():
                            file_name = result.get('metadata_storage_name', 'Unknown File')
                            path = result.get('metadata_storage_path', '')
                            
                            # í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬
                            highlights = result.get('@search.highlights')
                            if highlights:
                                # content ë˜ëŠ” content_exactì—ì„œ í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ
                                # ì—¬ëŸ¬ ê°œì˜ í•˜ì´ë¼ì´íŠ¸ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í•©ì³ì„œ ë³´ì—¬ì¤Œ
                                snippets = []
                                if 'content' in highlights:
                                    snippets.extend(highlights['content'])
                                if 'content_exact' in highlights:
                                    snippets.extend(highlights['content_exact'])
                                
                                # ì¤‘ë³µ ì œê±° ë° ê¸¸ì´ ì œí•œ
                                unique_snippets = list(set(snippets))[:3]
                                content_snippet = " ... ".join(unique_snippets)
                            else:
                                # í•˜ì´ë¼ì´íŠ¸ ì—†ìœ¼ë©´ ê¸°ë³¸ ìŠ¤ë‹ˆí«
                                content_snippet = result.get('content', '')[:300] + "..."
                            
                            blob_path = ""
                            try:
                                if CONTAINER_NAME in path:
                                    blob_path = path.split(f"/{CONTAINER_NAME}/")[-1]
                                    blob_path = urllib.parse.unquote(blob_path)
                            except:
                                pass
                                
                            st.markdown(f"### ğŸ“„ {file_name}")
                            st.markdown(f"> {content_snippet}", unsafe_allow_html=True) # HTML íƒœê·¸(bold) í—ˆìš©
                            
                            if blob_path:
                                try:
                                    blob_service_client = get_blob_service_client()
                                    
                                    # Content-Type ê²°ì • (í™•ì¥ì ìš°ì„  ì ìš©)
                                    # ë©”íƒ€ë°ì´í„°ê°€ application/octet-streamì¸ ê²½ìš°ê°€ ë§ì•„ í™•ì¥ìë¡œ ê°•ì œ ì„¤ì •
                                    if file_name.lower().endswith('.pdf'):
                                        content_type = "application/pdf"
                                    else:
                                        content_type = result.get('metadata_storage_content_type')
                                        if not content_type or content_type == "application/octet-stream":
                                            import mimetypes
                                            content_type, _ = mimetypes.guess_type(file_name)
                                    
                                    # Blob SAS ìƒì„± (Content-Disposition: inline ì„¤ì • + Content-Type ê°•ì œ)
                                    sas_token = generate_blob_sas(
                                        account_name=blob_service_client.account_name,
                                        container_name=CONTAINER_NAME,
                                        blob_name=blob_path,
                                        account_key=blob_service_client.credential.account_key,
                                        permission=BlobSasPermissions(read=True),
                                        expiry=datetime.utcnow() + timedelta(hours=1),
                                        content_disposition="inline", # ë¸Œë¼ìš°ì €ì—ì„œ ì—´ê¸° ê°•ì œ
                                        content_type=content_type # ì˜¬ë°”ë¥¸ MIME íƒ€ì… ì„¤ì •
                                    )
                                    
                                    sas_url = f"https://{blob_service_client.account_name}.blob.core.windows.net/{CONTAINER_NAME}/{urllib.parse.quote(blob_path)}?{sas_token}"
                                    
                                    # ìƒˆ íƒ­ì—ì„œ ì—´ê¸° (target="_blank")
                                    st.markdown(f'<a href="{sas_url}" target="_blank">ğŸ“„ ë¬¸ì„œ ì—´ê¸° (ìƒˆ íƒ­)</a>', unsafe_allow_html=True)
                                except Exception as e:
                                    st.caption(f"ë¬¸ì„œ ë§í¬ ìƒì„± ì‹¤íŒ¨: {e}")
                            
                            st.divider()
    
    with tab2:
        st.subheader("ğŸ¤– AI ë¬¸ì„œ ë„ìš°ë¯¸ (GPT-5.2)")
        st.caption("Azure OpenAI(GPT-5.2)ì™€ ë¬¸ì„œ ê²€ìƒ‰ì„ í™œìš©í•œ ì •í™•í•œ ë‹µë³€ ì œê³µ")
        
        # Initialize chat history in session state
        if "chat_messages" not in st.session_state:
            st.session_state.chat_messages = []
        
        # Display chat messages
        for message in st.session_state.chat_messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
                
                # Display citations if present
                if "citations" in message and message["citations"]:
                    st.markdown("---")
                    st.caption("ğŸ“š **ì°¸ì¡° ë¬¸ì„œ:**")
                    for i, citation in enumerate(message["citations"], 1):
                        filepath = citation.get('filepath', 'Unknown')
                        url = citation.get('url', '')
                        
                        # Generate SAS URL if we have blob path
                        if url:
                            display_url = url
                        else:
                            # Try to generate SAS URL from filepath
                            try:
                                blob_service_client = get_blob_service_client()
                                display_url = generate_sas_url(blob_service_client, CONTAINER_NAME, filepath)
                            except:
                                display_url = "#"
                        
                        st.markdown(f"{i}. [{filepath}]({display_url})")
        
        # -----------------------------
        # ê²€ìƒ‰ ì˜µì…˜ (Chat Tab) - Bottom of chat area
        # -----------------------------
        st.write("")
        with st.expander("âš™ï¸ ê³ ê¸‰ ê²€ìƒ‰ ì˜µì…˜ (RAG ì„¤ì •)", expanded=False):
            c1, c2 = st.columns(2)
            with c1:
                chat_use_semantic = st.checkbox("ì‹œë§¨í‹± ë­ì»¤ ì‚¬ìš©", value=False, key="chat_use_semantic", help="ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì—¬ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.")
            with c2:
                chat_search_mode_opt = st.radio("ê²€ìƒ‰ ëª¨ë“œ", ["all (AND)", "any (OR)"], index=1, horizontal=True, key="chat_search_mode", help="any: í‚¤ì›Œë“œ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ ê²€ìƒ‰ (ì¶”ì²œ)")
                chat_search_mode = "all" if "all" in chat_search_mode_opt else "any"

        # Chat input
        if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 10-P-101Aì˜ ì‚¬ì–‘ì€ ë¬´ì—‡ì¸ê°€ìš”?)"):
            # Add user message to chat history
            st.session_state.chat_messages.append({"role": "user", "content": prompt})
            
            # Display user message
            with st.chat_message("user"):
                st.markdown(prompt)
            
            # Get AI response
            with st.chat_message("assistant"):
                with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                    try:
                        chat_manager = get_chat_manager()
                        
                        # Prepare conversation history (exclude citations from history)
                        conversation_history = [
                            {"role": msg["role"], "content": msg["content"]}
                            for msg in st.session_state.chat_messages[:-1]  # Exclude the just-added user message
                        ]
                        
                        # Pass the selected search options to the chat manager
                        response_text, citations, context, final_filter, search_results = chat_manager.get_chat_response(
                            prompt, 
                            conversation_history, 
                            search_mode=chat_search_mode, 
                            use_semantic_ranker=chat_use_semantic
                        )
                        
                        # Display response
                        st.markdown(response_text)
                        
                        # Display citations
                        if citations:
                            st.markdown("---")
                            st.caption("ğŸ“š **ì°¸ì¡° ë¬¸ì„œ:**")
                            for i, citation in enumerate(citations, 1):
                                filepath = citation.get('filepath', 'Unknown')
                                url = citation.get('url', '')
                                
                                # Generate SAS URL if we have blob path
                                if url:
                                    display_url = url
                                else:
                                    # Try to generate SAS URL from filepath
                                    blob_service_client = get_blob_service_client()
                                    display_url = generate_sas_url(blob_service_client, CONTAINER_NAME, filepath)
                                
                                st.markdown(f"{i}. [{filepath}]({display_url})")
                        
                        # Add assistant response to chat history
                        st.session_state.chat_messages.append({
                            "role": "assistant",
                            "content": response_text,
                            "citations": citations,
                            "context": context,
                            "debug_filter": final_filter
                        })
                        
                        # Debug: Show Context
                        with st.expander("ğŸ” ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ í™•ì¸ (Debug Context)", expanded=False):
                            if final_filter:
                                st.caption(f"**OData Filter:** `{final_filter}`")
                            st.text_area("LLMì—ê²Œ ì „ë‹¬ëœ ì›ë¬¸ ë°ì´í„°", value=context, height=300)
                        
                    except Exception as e:
                        st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        
        # Clear chat button
        # Clear chat button
        if st.session_state.chat_messages:
            if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
                st.session_state.chat_messages = []
                st.rerun()

elif menu == "ë„ë©´/ìŠ¤í™ ë¹„êµ":
    # st.subheader("ğŸ—ï¸ ë„ë©´/ìŠ¤í™ ì •ë°€ ë¶„ì„ (RAG)") - Removed to avoid duplication
    
    tab1, tab2 = st.tabs(["ğŸ“¤ ë¬¸ì„œ ì—…ë¡œë“œ", "ğŸ’¬ AIë¶„ì„"])
    
    with tab1:
        
        if "drawing_uploader_key" not in st.session_state:
            st.session_state.drawing_uploader_key = 0
            
        # High Resolution OCR Toggle
        use_high_res = st.toggle("ê³ í•´ìƒë„ OCR ì ìš© (ë„ë©´ ë¯¸ì„¸ ê¸€ì ì¶”ì¶œìš©)", value=False, help="ë³µì¡í•œ ë„ë©´ì˜ ì‘ì€ ê¸€ì”¨ë¥¼ ë” ì •í™•í•˜ê²Œ ì½ìŠµë‹ˆë‹¤. ë¶„ì„ ì‹œê°„ì´ ë” ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        uploaded_files = st.file_uploader("PDF ë„ë©´, ìŠ¤í™, ì‚¬ì–‘ì„œ ë“±ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", accept_multiple_files=True, type=['pdf', 'png', 'jpg', 'jpeg', 'tiff', 'bmp'], key=f"drawing_{st.session_state.drawing_uploader_key}")
        
        if uploaded_files:
            if "analysis_status" not in st.session_state:
                st.session_state.analysis_status = {}
                
            if st.button("ì—…ë¡œë“œ ë° ë¶„ì„ ì‹œì‘"):
                blob_service_client = get_blob_service_client()
                container_client = blob_service_client.get_container_client(CONTAINER_NAME)
                doc_intel_manager = get_doc_intel_manager()
                search_manager = get_search_manager()
                
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                total_files = len(uploaded_files)
                
                for idx, file in enumerate(uploaded_files):
                    try:
                        # Normalize filename to NFC (to match search query logic)
                        import unicodedata
                        safe_filename = unicodedata.normalize('NFC', file.name)
                        
                        # Initialize status
                        st.session_state.analysis_status[safe_filename] = {
                            "status": "Extracting",
                            "total_pages": 0,
                            "processed_pages": 0,
                            "chunks": {},
                            "error": None
                        }
                        
                        status_text.text(f"ì²˜ë¦¬ ì¤‘ ({idx+1}/{total_files}): {safe_filename}")
                        
                        blob_path = f"{user_folder}/drawings/{safe_filename}"
                        # 2. Upload to Azure Blob Storage
                        status_text.text(f"ì—…ë¡œë“œ ì¤‘ ({idx+1}/{total_files}): {file.name}...")
                        blob_client = blob_service_client.get_blob_client(container=CONTAINER_NAME, blob=blob_path)
                        
                        # CRITICAL: Reset file pointer to ensure full upload
                        file.seek(0)
                        blob_client.upload_blob(file, overwrite=True)
                        
                        # Verify upload size
                        props = blob_client.get_blob_properties()
                        if props.size != file.size:
                            st.error(f"âš ï¸ íŒŒì¼ ì—…ë¡œë“œ í¬ê¸° ë¶ˆì¼ì¹˜! (ì›ë³¸: {file.size}, ì—…ë¡œë“œë¨: {props.size})")
                        else:
                            print(f"DEBUG: Upload verified. Size: {props.size} bytes")

                        # Generate SAS Token for Document Intelligence access
                        sas_token = generate_blob_sas(
                            account_name=blob_service_client.account_name,
                            container_name=CONTAINER_NAME,
                            blob_name=blob_path,
                            account_key=blob_service_client.credential.account_key,
                            permission=BlobSasPermissions(read=True),
                            expiry=datetime.utcnow() + timedelta(hours=1)
                        )
                        blob_url = f"https://{blob_service_client.account_name}.blob.core.windows.net/{CONTAINER_NAME}/{urllib.parse.quote(blob_path)}?{sas_token}"
                        
                        # 3. Analyze with Document Intelligence (Chunked)
                        file.seek(0)
                        pdf_data = file.read()
                        doc = fitz.open(stream=pdf_data, filetype="pdf")
                        total_pages = doc.page_count
                        file.seek(0)
                        
                        status_text.text(f"ë¶„ì„ ì¤€ë¹„ ì¤‘ ({idx+1}/{total_files}): {file.name} (ì´ {total_pages} í˜ì´ì§€)")
                        
                        st.session_state.analysis_status[safe_filename]["total_pages"] = total_pages
                        
                        chunk_size = 50
                        page_chunks = []
                        
                        for start_page in range(1, total_pages + 1, chunk_size):
                            end_page = min(start_page + chunk_size - 1, total_pages)
                            page_range = f"{start_page}-{end_page}"
                            
                            st.session_state.analysis_status[safe_filename]["chunks"][page_range] = "Extracting"
                            status_text.text(f"ë¶„ì„ ì¤‘ ({idx+1}/{total_files}): {file.name} - í˜ì´ì§€ {page_range} ë¶„ì„ ì¤‘...")
                            
                            # Retry logic for each chunk
                            max_retries = 3
                            for retry in range(max_retries):
                                try:
                                    chunks = doc_intel_manager.analyze_document(blob_url, page_range=page_range, high_res=use_high_res)
                                    page_chunks.extend(chunks)
                                    st.session_state.analysis_status[safe_filename]["chunks"][page_range] = "Ready"
                                    st.session_state.analysis_status[safe_filename]["processed_pages"] += len(chunks)
                                    break
                                except Exception as e:
                                    if retry == max_retries - 1:
                                        st.session_state.analysis_status[safe_filename]["chunks"][page_range] = "Failed"
                                        st.session_state.analysis_status[safe_filename]["error"] = str(e)
                                        raise e
                                    
                                    # Transient error - show friendly message
                                    wait_time = 5 * (retry + 1)
                                    status_text.text(f"â³ ì¼ì‹œì  ì§€ì—°ìœ¼ë¡œ ì¬ì—°ê²° ì¤‘ ({retry+1}/{max_retries}): {file.name} - í˜ì´ì§€ {page_range} (ì•½ {wait_time}ì´ˆ ëŒ€ê¸°)...")
                                    time.sleep(wait_time)
                        
                        # 4. Indexing
                        st.session_state.analysis_status[safe_filename]["status"] = "Indexing"
                        
                        if len(page_chunks) == 0:
                            st.warning(f"âš ï¸ ê²½ê³ : '{file.name}'ì—ì„œ í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        
                        documents_to_index = []
                        for page_chunk in page_chunks:
                            # Create document object for each page
                            # ID must be unique and URL safe. Include page number in ID.
                            import base64
                            page_id_str = f"{blob_path}_page_{page_chunk['page_number']}"
                            doc_id = base64.urlsafe_b64encode(page_id_str.encode('utf-8')).decode('utf-8')
                            
                            document = {
                                "id": doc_id,
                                "content": page_chunk['content'],
                                "content_exact": page_chunk['content'],
                                "metadata_storage_name": f"{safe_filename} (p.{page_chunk['page_number']})",
                                "metadata_storage_path": f"https://{blob_service_client.account_name}.blob.core.windows.net/{CONTAINER_NAME}/{blob_path}#page={page_chunk['page_number']}",
                                "metadata_storage_last_modified": datetime.utcnow().isoformat() + "Z",
                                "metadata_storage_size": file.size,
                                "metadata_storage_content_type": file.type,
                                "project": "drawings_analysis",  # Tag for filtering
                                "page_number": page_chunk['page_number'],
                                "filename": safe_filename
                            }
                            documents_to_index.append(document)
                        
                        # Batch upload all pages (50 pages at a time to avoid request size limits)
                        if documents_to_index:
                            batch_size = 50
                            for i in range(0, len(documents_to_index), batch_size):
                                batch = documents_to_index[i:i + batch_size]
                                status_text.text(f"ì¸ë±ì‹± ì¤‘ ({idx+1}/{total_files}): {safe_filename} - ë°°ì¹˜ ì „ì†¡ ì¤‘ ({i//batch_size + 1}/{(len(documents_to_index)-1)//batch_size + 1})")
                                success, msg = search_manager.upload_documents(batch)
                                if not success:
                                    st.error(f"ì¸ë±ì‹± ì‹¤íŒ¨ ({file.name}, ë°°ì¹˜ {i//batch_size + 1}): {msg}")
                                    break
                            
                            # 5. Save Analysis JSON to Blob Storage (Dual Retrieval Strategy)
                            # This allows exact retrieval by filename without AI Search
                            status_text.text(f"ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘ ({idx+1}/{total_files}): {safe_filename}...")
                            search_manager.upload_analysis_json(container_client, user_folder, safe_filename, page_chunks)
                        
                        st.session_state.analysis_status[safe_filename]["status"] = "Ready"
                        progress_bar.progress((idx + 1) / total_files)
                        
                    except Exception as e:
                        st.error(f"ì˜¤ë¥˜ ë°œìƒ ({file.name}): {str(e)}")
                
                status_text.text("ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.success("ì—…ë¡œë“œ, ë¶„ì„ ë° ì¸ë±ì‹±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                # ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ë©´ ì—…ë¡œë” ì´ˆê¸°í™”
                st.session_state.drawing_uploader_key += 1
                time.sleep(2)
                st.rerun()

        # ğŸ“Š ë¶„ì„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
        if "analysis_status" in st.session_state and st.session_state.analysis_status:
            st.divider()
            st.markdown("#### ğŸ“Š ë¶„ì„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ")
            for filename, info in st.session_state.analysis_status.items():
                status_color = "green" if info['status'] == "Ready" else "orange" if info['status'] != "Failed" else "red"
                with st.expander(f":{status_color}[{filename}] - {info['status']}", expanded=(info['status'] != "Ready")):
                    col1, col2 = st.columns([3, 1])
                    with col1:
                        st.write(f"**ì „ì²´ ìƒíƒœ:** {info['status']}")
                        progress = info['processed_pages'] / info['total_pages'] if info['total_pages'] > 0 else 0
                        st.progress(progress)
                        st.write(f"**ì§„í–‰ë„:** {info['processed_pages']} / {info['total_pages']} í˜ì´ì§€ ì™„ë£Œ")
                    
                    if info['error']:
                        st.error(f"**ìµœê·¼ ì˜¤ë¥˜:** {info['error']}")
                    
                    # ì„¸ë¶€ ì²­í¬ ìƒíƒœ
                    if info['chunks']:
                        st.markdown("---")
                        st.caption("ğŸ§© **í˜ì´ì§€ ì²­í¬ë³„ ìƒíƒœ**")
                        chunk_cols = st.columns(4)
                        for i, (chunk_range, chunk_status) in enumerate(info['chunks'].items()):
                            with chunk_cols[i % 4]:
                                if chunk_status == "Ready":
                                    st.success(f"âœ… {chunk_range}")
                                elif chunk_status == "Failed":
                                    st.error(f"âŒ {chunk_range}")
                                    # ì¬ì‹œë„ ë²„íŠ¼ (ê°„ì†Œí™”ëœ êµ¬í˜„)
                                    if st.button("ğŸ”„", key=f"retry_{filename}_{chunk_range}", help=f"{chunk_range} ì¬ì‹œë„"):
                                        st.info("ì¬ì‹œë„ëŠ” 'ì—…ë¡œë“œ ë° ë¶„ì„ ì‹œì‘'ì„ ë‹¤ì‹œ ëˆŒëŸ¬ì£¼ì„¸ìš” (ë©±ë“±ì„± ë³´ì¥)")
                                else:
                                    st.info(f"â³ {chunk_range}")

    with tab2:

        
        # Display analyzed documents
        st.markdown("#### ğŸ“‹ ë¶„ì„ëœ ë¬¸ì„œ ëª©ë¡")
        try:
            blob_service_client = get_blob_service_client()
            container_client = blob_service_client.get_container_client(CONTAINER_NAME)
            
            # List files in user's drawings folder + Admin access to root drawings
            blobs = []
            # User folder
            blobs.extend(list(container_client.list_blobs(name_starts_with=f"{user_folder}/drawings/")))
            
            if user_role == 'admin':
                # Admin root folder
                blobs.extend(list(container_client.list_blobs(name_starts_with="drawings/")))
            
            # Deduplicate
            unique_blobs = {b.name: b for b in blobs}.values()
            
            blob_list = []
            available_filenames = []
            for blob in unique_blobs:
                if not blob.name.endswith('/'):  # Skip folder markers
                    filename = blob.name.split('/')[-1]
                    blob_list.append({
                        'name': filename,
                        'full_name': blob.name,
                        'size': blob.size,
                        'modified': blob.last_modified
                    })
                    available_filenames.append(filename)
            
            # Sort by modified date (most recent first)
            blob_list.sort(key=lambda x: x['modified'], reverse=True)
            
            selected_filenames = []
            
            if blob_list:
                st.info(f"ì´ {len(blob_list)}ê°œì˜ ë¬¸ì„œê°€ ë¶„ì„ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë¶„ì„í•  ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš”.")
                
                # Add "Select All" checkbox
                def toggle_all():
                    new_state = st.session_state.select_all_files
                    # Update state for ALL files in the list, not just existing keys
                    for b in blob_list:
                        st.session_state[f"chk_{b['name']}"] = new_state

                select_all = st.checkbox("ì „ì²´ ì„ íƒ", value=True, key="select_all_files", on_change=toggle_all)
                
                # Display as expandable list
                with st.expander("ğŸ“„ ë¬¸ì„œ ëª©ë¡ ë° ì„ íƒ", expanded=True):
                    for idx, blob_info in enumerate(blob_list, 1):
                        col0, col1, col2, col3 = st.columns([0.5, 4, 1.2, 1])
                        with col0:
                            # Checkbox for selection
                            # Initialize state if missing
                            chk_key = f"chk_{blob_info['name']}"
                            if chk_key not in st.session_state:
                                st.session_state[chk_key] = True # Default to True (Select All default)
                                
                            is_selected = st.checkbox(f"select_{idx}", key=chk_key, label_visibility="collapsed")
                            if is_selected:
                                selected_filenames.append(blob_info['name'])
                        
                        with col1:
                            size_mb = blob_info['size'] / (1024 * 1024)
                            modified_str = blob_info['modified'].strftime('%Y-%m-%d %H:%M')
                            st.markdown(f"**{blob_info['name']}** ({size_mb:.2f} MB)")
                        
                        with col2:
                            # Use sub-columns to align icons horizontally
                            sub_c1, sub_c2, sub_c3 = st.columns([1, 1, 1])
                            
                            with sub_c1:
                                # 1. Download Button
                                try:
                                    sas_url = generate_sas_url(
                                        blob_service_client, 
                                        CONTAINER_NAME, 
                                        blob_info['full_name'], 
                                        content_disposition="attachment"
                                    )
                                    # Use st.link_button for consistent UI (Box style)
                                    st.link_button("ğŸ“¥", sas_url, help="ë‹¤ìš´ë¡œë“œ", use_container_width=True)
                                except Exception as e:
                                    st.error(f"Err: {e}")

                            with sub_c2:
                                # 2. Rename Button (Popover)
                                # Popover button is wide by default, try to make it compact?
                                # Streamlit buttons expand to column width.
                                with st.popover("âœï¸", use_container_width=True):
                                    new_name_input = st.text_input("ìƒˆ íŒŒì¼ëª…", value=blob_info['name'], key=f"ren_{blob_info['name']}")
                                    if st.button("ì´ë¦„ ë³€ê²½", key=f"btn_ren_{blob_info['name']}"):
                                        if new_name_input != blob_info['name']:
                                            try:
                                                with st.spinner("ì´ë¦„ ë³€ê²½ ë° ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì¤‘..."):
                                                    # A. Rename Blob
                                                    old_blob_name = blob_info['full_name']
                                                    folder_path = old_blob_name.rsplit('/', 1)[0]
                                                    new_blob_name = f"{folder_path}/{new_name_input}"
                                                    
                                                    source_blob = container_client.get_blob_client(old_blob_name)
                                                    dest_blob = container_client.get_blob_client(new_blob_name)
                                                    
                                                    # Copy
                                                    source_sas = generate_sas_url(blob_service_client, CONTAINER_NAME, old_blob_name)
                                                    dest_blob.start_copy_from_url(source_sas)
                                                    
                                                    # Wait for copy
                                                    for _ in range(20):
                                                        props = dest_blob.get_blob_properties()
                                                        if props.copy.status == "success":
                                                            break
                                                        time.sleep(0.2)
                                                    
                                                    # B. Update Search Index (Preserve OCR Data)
                                                    search_manager = get_search_manager()
                                                    import unicodedata
                                                    safe_old_filename = unicodedata.normalize('NFC', blob_info['name'])
                                                    safe_new_filename = unicodedata.normalize('NFC', new_name_input)
                                                    
                                                    # Find old docs
                                                    results = search_manager.search_client.search(
                                                        search_text="*",
                                                        filter=f"project eq 'drawings_analysis'",
                                                        select=["id", "content", "content_exact", "metadata_storage_name", "metadata_storage_path", "metadata_storage_size", "metadata_storage_content_type"]
                                                    )
                                                    
                                                    docs_to_upload = []
                                                    ids_to_delete = []
                                                    
                                                    for doc in results:
                                                        # Check if this doc belongs to the file (by name prefix)
                                                        # Name format: "{filename} (p.{page})"
                                                        if doc['metadata_storage_name'].startswith(safe_old_filename):
                                                            # Create new doc
                                                            page_suffix = doc['metadata_storage_name'].split(safe_old_filename)[-1] # e.g. " (p.1)"
                                                            
                                                            # New ID
                                                            import base64
                                                            # Extract page number from suffix or path if possible, or just reconstruct
                                                            # Path format: .../filename#page=N
                                                            try:
                                                                page_num = doc['metadata_storage_path'].split('#page=')[-1]
                                                                new_page_id_str = f"{new_blob_name}_page_{page_num}"
                                                                new_doc_id = base64.urlsafe_b64encode(new_page_id_str.encode('utf-8')).decode('utf-8')
                                                                
                                                                new_doc = {
                                                                    "id": new_doc_id,
                                                                    "content": doc['content'],
                                                                    "content_exact": doc.get('content_exact', doc['content']),
                                                                    "metadata_storage_name": f"{safe_new_filename}{page_suffix}",
                                                                    "metadata_storage_path": f"https://{blob_service_client.account_name}.blob.core.windows.net/{CONTAINER_NAME}/{new_blob_name}#page={page_num}",
                                                                    "metadata_storage_last_modified": datetime.utcnow().isoformat() + "Z",
                                                                    "metadata_storage_size": doc['metadata_storage_size'],
                                                                    "metadata_storage_content_type": doc['metadata_storage_content_type'],
                                                                    "project": "drawings_analysis"
                                                                }
                                                                docs_to_upload.append(new_doc)
                                                                ids_to_delete.append({"id": doc['id']})
                                                            except:
                                                                pass

                                                    if docs_to_upload:
                                                        search_manager.upload_documents(docs_to_upload)
                                                    if ids_to_delete:
                                                        search_manager.search_client.delete_documents(documents=ids_to_delete)

                                                    # C. Delete old blob
                                                    source_blob.delete_blob()
                                                    
                                                    st.success("ì´ë¦„ ë³€ê²½ ì™„ë£Œ!")
                                                    time.sleep(1)
                                                    st.rerun()
                                                    
                                            except Exception as e:
                                                st.error(f"ë³€ê²½ ì‹¤íŒ¨: {e}")

                            with sub_c3:
                                # 3. Re-analyze Button
                                if st.button("ğŸ”„", key=f"reanalyze_{blob_info['name']}", help="ì¬ë¶„ì„ (ì¸ë±ìŠ¤ ë³µêµ¬)"):
                                    try:
                                        with st.spinner("ì¬ë¶„ì„ ì‹œì‘... (íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘)"):
                                            # A. Download Blob to memory
                                            blob_client = container_client.get_blob_client(blob_info['full_name'])
                                            download_stream = blob_client.download_blob()
                                            pdf_data = download_stream.readall()
                                            
                                            # B. Count Pages
                                            import fitz
                                            doc = fitz.open(stream=pdf_data, filetype="pdf")
                                            total_pages = doc.page_count
                                            
                                            # C. Initialize Status
                                            if "analysis_status" not in st.session_state:
                                                st.session_state.analysis_status = {}
                                            
                                            safe_filename = blob_info['name']
                                            st.session_state.analysis_status[safe_filename] = {
                                                "status": "Extracting",
                                                "total_pages": total_pages,
                                                "processed_pages": 0,
                                                "chunks": {},
                                                "error": None
                                            }
                                            
                                            # D. Analyze Chunks
                                            doc_intel_manager = get_doc_intel_manager()
                                            search_manager = get_search_manager()
                                            blob_service_client = get_blob_service_client()
                                            
                                            # Generate SAS for Analysis
                                            sas_token = generate_blob_sas(
                                                account_name=blob_service_client.account_name,
                                                container_name=CONTAINER_NAME,
                                                blob_name=blob_info['full_name'],
                                                account_key=blob_service_client.credential.account_key,
                                                permission=BlobSasPermissions(read=True),
                                                expiry=datetime.utcnow() + timedelta(hours=1)
                                            )
                                            # Use relative path for URL construction if needed, but full_name is usually relative to container if listed from container_client?
                                            # container_client.list_blobs returns name relative to container.
                                            blob_url = f"https://{blob_service_client.account_name}.blob.core.windows.net/{CONTAINER_NAME}/{urllib.parse.quote(blob_info['full_name'])}?{sas_token}"
                                            
                                            chunk_size = 50
                                            page_chunks = []
                                            
                                            progress_bar = st.progress(0)
                                            status_text = st.empty()
                                            
                                            for start_page in range(1, total_pages + 1, chunk_size):
                                                end_page = min(start_page + chunk_size - 1, total_pages)
                                                page_range = f"{start_page}-{end_page}"
                                                
                                                st.session_state.analysis_status[safe_filename]["chunks"][page_range] = "Extracting"
                                                status_text.text(f"ì¬ë¶„ì„ ì¤‘: {safe_filename} ({page_range})...")
                                                
                                                # Retry logic
                                                max_retries = 3
                                                for retry in range(max_retries):
                                                    try:
                                                        chunks = doc_intel_manager.analyze_document(blob_url, page_range=page_range, high_res=use_high_res)
                                                        page_chunks.extend(chunks)
                                                        st.session_state.analysis_status[safe_filename]["chunks"][page_range] = "Ready"
                                                        st.session_state.analysis_status[safe_filename]["processed_pages"] += len(chunks)
                                                        break
                                                    except Exception as e:
                                                        if retry == max_retries - 1:
                                                            st.session_state.analysis_status[safe_filename]["chunks"][page_range] = "Failed"
                                                            st.session_state.analysis_status[safe_filename]["error"] = str(e)
                                                            raise e
                                                        time.sleep(5 * (retry + 1))
                                            
                                            # E. Indexing
                                            st.session_state.analysis_status[safe_filename]["status"] = "Indexing"
                                            status_text.text("ì¸ë±ì‹± ì¤‘...")
                                            
                                            documents_to_index = []
                                            for page_chunk in page_chunks:
                                                import base64
                                                # Use full_name (path in container) for ID generation to match upload logic
                                                page_id_str = f"{blob_info['full_name']}_page_{page_chunk['page_number']}"
                                                doc_id = base64.urlsafe_b64encode(page_id_str.encode('utf-8')).decode('utf-8')
                                                
                                                document = {
                                                    "id": doc_id,
                                                    "content": page_chunk['content'],
                                                    "content_exact": page_chunk['content'],
                                                    "metadata_storage_name": f"{safe_filename} (p.{page_chunk['page_number']})",
                                                    "metadata_storage_path": f"https://{blob_service_client.account_name}.blob.core.windows.net/{CONTAINER_NAME}/{blob_info['full_name']}#page={page_chunk['page_number']}",
                                                    "metadata_storage_last_modified": datetime.utcnow().isoformat() + "Z",
                                                    "metadata_storage_size": blob_info['size'],
                                                    "metadata_storage_content_type": "application/pdf",
                                                    "project": "drawings_analysis",
                                                    "page_number": page_chunk['page_number'],
                                                    "filename": safe_filename
                                                }
                                                documents_to_index.append(document)
                                            
                                            if documents_to_index:
                                                batch_size = 50
                                                for i in range(0, len(documents_to_index), batch_size):
                                                    batch = documents_to_index[i:i + batch_size]
                                                    search_manager.upload_documents(batch)
                                                
                                                # Save JSON
                                                search_manager.upload_analysis_json(container_client, user_folder, safe_filename, page_chunks)
                                            
                                            st.session_state.analysis_status[safe_filename]["status"] = "Ready"
                                            st.success("ì¬ë¶„ì„ ì™„ë£Œ! ì´ì œ ê²€ìƒ‰ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                                            time.sleep(1)
                                            st.rerun()

                                    except Exception as e:
                                        st.error(f"ì¬ë¶„ì„ ì‹¤íŒ¨: {e}")

                            # 3. JSON (Admin only)
                            if user_role == 'admin':
                                json_key = f"json_data_{blob_info['name']}"
                                
                                if json_key not in st.session_state:
                                    if st.button("JSON", key=f"gen_json_{blob_info['name']}"):
                                        with st.spinner("..."):
                                            search_manager = get_search_manager()
                                            # Dual Retrieval Strategy: Try Blob first
                                            docs = search_manager.get_document_json_from_blob(container_client, user_folder, blob_info['name'])
                                            
                                            # Fallback to AI Search if Blob JSON not found (for older files)
                                            if not docs:
                                                st.info("Blob JSONì„ ì°¾ì„ ìˆ˜ ì—†ì–´ AI Searchì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
                                                docs = search_manager.get_document_json(blob_info['name'])
                                                
                                            if docs:
                                                import json
                                                json_str = json.dumps(docs, ensure_ascii=False, indent=2)
                                                st.session_state[json_key] = json_str
                                                st.rerun()
                                            else:
                                                st.error(f"No Data found for '{blob_info['name']}'")
                                                # Try one more time without project filter to see if it exists at all
                                                safe_name = blob_info['name'].replace("'", "''")
                                                debug_docs = search_manager.search_client.search(
                                                    search_text="*",
                                                    filter=f"search.ismatch('\"{safe_name}*\"', 'metadata_storage_name')",
                                                    select=["metadata_storage_name", "project"],
                                                    top=5
                                                )
                                                debug_list = list(debug_docs)
                                                if debug_list:
                                                    st.warning(f"Found {len(debug_list)} docs without correct project tag. Example: {debug_list[0].get('metadata_storage_name')} (Project: {debug_list[0].get('project')})")
                                                else:
                                                    st.error("Document not found in index at all.")
                                else:
                                    # Show download button
                                    json_data = st.session_state[json_key]
                                    st.download_button(
                                        label="ğŸ’¾",
                                        data=json_data,
                                        file_name=f"{blob_info['name']}.json",
                                        mime="application/json",
                                        key=f"dl_json_{blob_info['name']}"
                                    )

                        with col3:
                            if st.button("ğŸ—‘ï¸ ì‚­ì œ", key=f"del_{blob_info['name']}"):
                                try:
                                    # 1. Delete from Blob Storage (Use full_name)
                                    blob_client = container_client.get_blob_client(blob_info['full_name'])
                                    blob_client.delete_blob()
                                    
                                    # 2. Delete from Search Index
                                    search_manager = get_search_manager()
                                    
                                    # Find docs to delete
                                    import unicodedata
                                    safe_filename = unicodedata.normalize('NFC', blob_info['name'])
                                    
                                    # Clean up index (Find ALL pages)
                                    ids_to_delete = []
                                    while True:
                                        results = search_manager.search_client.search(
                                            search_text="*",
                                            filter=f"project eq 'drawings_analysis'",
                                            select=["id", "metadata_storage_name"],
                                            top=1000
                                        )
                                        
                                        batch_ids = []
                                        for doc in results:
                                            # Use NFC normalization for comparison
                                            doc_name = unicodedata.normalize('NFC', doc['metadata_storage_name'])
                                            if doc_name.startswith(safe_filename):
                                                batch_ids.append({"id": doc['id']})
                                        
                                        if not batch_ids:
                                            break
                                            
                                        search_manager.search_client.delete_documents(documents=batch_ids)
                                        ids_to_delete.extend(batch_ids)
                                        
                                        # If we found less than 1000, we might be done, but to be safe we continue 
                                        # until a search returns no matches for our file.
                                        # Actually, if we delete them, the next search will return different docs.
                                        # So we continue until no more docs match.
                                        if len(batch_ids) == 0:
                                            break
                                    
                                    # Clear JSON state if exists
                                    json_key = f"json_data_{blob_info['name']}"
                                    if json_key in st.session_state:
                                        del st.session_state[json_key]

                                    st.success(f"{blob_info['name']} ì‚­ì œ ì™„ë£Œ")
                                    st.rerun()
                                except Exception as e:
                                    st.error(f"ì‚­ì œ ì‹¤íŒ¨: {e}")
            else:
                st.warning("ë¶„ì„ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. 'ë¬¸ì„œ ì—…ë¡œë“œ ë° ë¶„ì„' íƒ­ì—ì„œ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        except Exception as e:
            st.error(f"ë¬¸ì„œ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        st.divider()
        
        # DEBUG: Show selected files
        st.write(f"DEBUG: Selected Files ({len(selected_filenames)}): {selected_filenames}")
        
        # Chat Interface (Similar to main chat but focused)
        if "rag_chat_messages" not in st.session_state:
            st.session_state.rag_chat_messages = []
            
        for message in st.session_state.rag_chat_messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
                if "citations" in message and message["citations"]:
                    st.markdown("---")
                    st.caption("ğŸ“š **ì°¸ì¡° ë¬¸ì„œ:**")
                    for i, citation in enumerate(message["citations"], 1):
                        filepath = citation.get('filepath', 'Unknown')
                        url = citation.get('url', '')
                        
                        # Generate SAS URL for browser viewing
                        if url:
                            display_url = url
                        else:
                            try:
                                blob_service_client = get_blob_service_client()
                                # Generate SAS with inline content disposition
                                sas_token = generate_blob_sas(
                                    account_name=blob_service_client.account_name,
                                    container_name=CONTAINER_NAME,
                                    blob_name=filepath,
                                    account_key=blob_service_client.credential.account_key,
                                    permission=BlobSasPermissions(read=True),
                                    expiry=datetime.utcnow() + timedelta(hours=1),
                                    content_disposition="inline",
                                    content_type="application/pdf"
                                )
                                display_url = f"https://{blob_service_client.account_name}.blob.core.windows.net/{CONTAINER_NAME}/{urllib.parse.quote(filepath)}?{sas_token}"
                                
                                # Add page number if available
                                page_num = citation.get('page')
                                if page_num:
                                    display_url += f"#page={page_num}"
                            except:
                                display_url = "#"
                        
                        st.markdown(f"{i}. [{filepath}]({display_url})")

        if prompt := st.chat_input("ë„ë©´ì´ë‚˜ ìŠ¤í™ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”..."):
            st.session_state.rag_chat_messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            
            with st.chat_message("assistant"):
                with st.spinner("ë¶„ì„ ì¤‘..."):
                    try:
                        chat_manager = get_chat_manager()
                        
                        conversation_history = [
                            {"role": msg["role"], "content": msg["content"]}
                            for msg in st.session_state.rag_chat_messages[:-1]
                        ]
                        
                        # Use 'any' search mode for better recall (find documents even with partial keyword match)
                        # This is important because technical drawings may have specific terms
                        # Filter to only search documents from the drawings folder
                        # Pass selected_filenames for specific file filtering
                        # If selected_filenames is empty (user deselected all), we should probably warn or search nothing.
                        # But for now let's pass it. If empty, the chat manager might search nothing or all depending on logic.
                        # Actually, let's default to all if none selected? No, user explicitly deselected.
                        # Let's pass the list as is.
                        
                        # Note: selected_filenames is defined in the outer scope of the tab
                        current_files = selected_filenames
                        
                        # Construct robust filter expression
                        # Include fallback for documents that might have lost their project tag but are in the drawings folder
                        base_filter = "(project eq 'drawings_analysis' or search.ismatch('/drawings/', 'metadata_storage_path'))"
                        
                        # Note: We used to filter by path here, but OData encoding issues caused 0 results.
                        # Now we pass user_folder to chat_manager for Python-side filtering.

                        response_text, citations, context, final_filter, search_results = chat_manager.get_chat_response(
                            prompt, 
                            conversation_history,
                            search_mode="any",
                            use_semantic_ranker=False,
                            filter_expr=base_filter,
                            available_files=current_files,
                            user_folder=user_folder # Pass user folder for Python-side filtering
                        )
                        
                        st.markdown(response_text)
                        
                        # Display Google-like search results (Snippets + Links)
                        if search_results:
                            with st.expander("ğŸ” ê²€ìƒ‰ ê²°ê³¼ ë° ìŠ¤ë‹ˆí« (ìƒìœ„ í›„ë³´)", expanded=True):
                                for i, res in enumerate(search_results[:5]): # Show top 5 for clarity
                                    res_name = res.get('metadata_storage_name', 'Unknown')
                                    res_path = res.get('metadata_storage_path', '')
                                    
                                    # Extract snippet from highlights
                                    highlights = res.get('@search.highlights', {})
                                    snippet = highlights.get('content', [""])[0] if highlights else ""
                                    if not snippet:
                                        snippet = res.get('content', '')[:200] + "..."
                                    
                                    # Generate SAS link for the result
                                    try:
                                        # Extract blob path from metadata_storage_path
                                        from urllib.parse import unquote
                                        import re
                                        
                                        if "https://direct_fetch/" in res_path:
                                            # Handle custom direct fetch scheme
                                            path_without_scheme = res_path.replace("https://direct_fetch/", "")
                                            blob_path_part = path_without_scheme.split('#')[0]
                                            blob_path_part = unquote(blob_path_part)
                                        elif CONTAINER_NAME in res_path:
                                            # Handle standard Azure Blob URL
                                            blob_path_part = res_path.split(f"/{CONTAINER_NAME}/")[1].split('#')[0]
                                            blob_path_part = unquote(blob_path_part)
                                        else:
                                            # Fallback or relative path
                                            blob_path_part = res_path
                                        
                                        # CRITICAL FIX: Strip " (p.N)" suffix if present in the path
                                        # This happens if the indexer appended it to the path
                                        blob_path_part = re.sub(r'\s*\(p\.\d+\)$', '', blob_path_part)
                                            
                                        sas_url = chat_manager.generate_sas_url(blob_path_part)
                                    except:
                                        sas_url = "#"

                                    st.markdown(f"**{i+1}. {res_name}**")
                                    st.write(f"_{snippet}_")
                                    if sas_url != "#":
                                        st.markdown(f"[ğŸ“¥ ì›ë³¸ ë‹¤ìš´ë¡œë“œ]({sas_url})")
                                    st.divider()

                        if citations:
                            st.markdown("---")
                            st.caption("ğŸ“š **ì°¸ì¡° ë¬¸ì„œ:**")
                            for i, citation in enumerate(citations, 1):
                                filepath = citation.get('filepath', 'Unknown')
                                url = citation.get('url', '')
                                
                                # Generate SAS URL for browser viewing
                                if url:
                                    display_url = url
                                else:
                                    try:
                                        blob_service_client = get_blob_service_client()
                                        # Generate SAS with inline content disposition
                                        sas_token = generate_blob_sas(
                                            account_name=blob_service_client.account_name,
                                            container_name=CONTAINER_NAME,
                                            blob_name=filepath,
                                            account_key=blob_service_client.credential.account_key,
                                            permission=BlobSasPermissions(read=True),
                                            expiry=datetime.utcnow() + timedelta(hours=1),
                                            content_disposition="inline",
                                            content_type="application/pdf"
                                        )
                                        display_url = f"https://{blob_service_client.account_name}.blob.core.windows.net/{CONTAINER_NAME}/{urllib.parse.quote(filepath)}?{sas_token}"
                                        
                                        # Add page number if available
                                        page_num = citation.get('page')
                                        if page_num:
                                            display_url += f"#page={page_num}"
                                    except:
                                        display_url = "#"
                                
                                st.markdown(f"{i}. [{filepath}]({display_url})")
                        
                        # Debug: Show Context
                        with st.expander("ğŸ” ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ í™•ì¸ (Debug Context)", expanded=False):
                            if final_filter:
                                st.caption(f"**OData Filter:** `{final_filter}`")
                            if search_results:
                                st.caption(f"**Search Results:** {len(search_results)} chunks found")
                            st.text_area("LLMì—ê²Œ ì „ë‹¬ëœ ì›ë¬¸ ë°ì´í„°", value=context, height=300)

                        st.session_state.rag_chat_messages.append({
                            "role": "assistant",
                            "content": response_text,
                            "citations": citations,
                            "context": context
                        })
                        st.rerun()


                    except Exception as e:
                        st.error(f"ì˜¤ë¥˜: {e}")
                        import traceback
                        st.code(traceback.format_exc())

elif menu == "ë””ë²„ê·¸ (Debug)":
    st.title("ğŸ•µï¸â€â™‚ï¸ RAG Deep Diagnostic Tool (Integrated)")
    
    # Check if admin
    if user_role != 'admin':
        st.error("Admin access required.")
        st.stop()

    search_manager = get_search_manager()
    blob_service_client = get_blob_service_client()
    container_client = blob_service_client.get_container_client(CONTAINER_NAME)

    # Fetch list of files for selection (Filter for drawings only)
    blob_list = []
    try:
        blobs = container_client.list_blobs()
        for b in blobs:
            # Filter: Must be a file (not folder) AND must be in a 'drawings' folder
            if not b.name.endswith('/') and '/drawings/' in b.name:
                blob_list.append(b.name)
    except Exception as e:
        st.error(f"Failed to list blobs: {e}")
    
    blob_list.sort(key=lambda x: x.split('/')[-1]) # Sort by filename
    
    target_blob = st.selectbox("Select Target File", blob_list)
    
    # Extract filename for search
    if target_blob:
        filename = target_blob.split('/')[-1]
        st.caption(f"Selected Filename for Search: `{filename}`")
    else:
        filename = st.text_input("Target Filename", value="ì œ4ê¶Œ ë„ë©´(ì²­ì£¼).pdf")

    if st.button("Run Diagnostics"):
        st.divider()
        
        # 1. Index Inspection
        st.subheader("1. Index Inspection")
        
        # Search for ALL pages
        try:
            # Attempt 1: Exact Match Filter (Most accurate)
            safe_filename = filename.replace("'", "''")
            results = search_manager.search_client.search(
                search_text="*",
                filter=f"metadata_storage_name eq '{safe_filename}'",
                select=["id", "metadata_storage_name", "metadata_storage_path", "project", "content"],
                top=50
            )
            results = list(results)
        except Exception as e:
            st.warning(f"Exact match filter failed ({str(e)}). Switching to fallback search...")
            # Attempt 2: Fallback Text Search (Broader)
            # Search for the filename as a phrase
            results = search_manager.search_client.search(
                search_text=f"\"{filename}\"",
                search_mode="all",
                select=["id", "metadata_storage_name", "metadata_storage_path", "project", "content"],
                top=100
            )
            # Filter client-side to ensure we get the file AND its chunks
            # Allow exact match OR "filename (p.N)" format
            import unicodedata
            norm_filename = unicodedata.normalize('NFC', filename)
            results = [
                doc for doc in results 
                if unicodedata.normalize('NFC', doc['metadata_storage_name']).startswith(norm_filename)
            ]
        
        st.write(f"Found **{len(results)}** documents in index.")
        
        if results:
            # Analyze First Result
            first = results[0]
            st.json({
                "First Doc ID": first['id'],
                "Name": first['metadata_storage_name'],
                "Path": first['metadata_storage_path'],
                "Project": first['project']
            })
        else:
            st.error("No documents found in index matching this filename.")
            
            # Debug: List what IS in the index
            st.divider()
            st.subheader("ğŸ•µï¸ Index Content Peek (Top 20)")
            try:
                # Get top 20 docs to see what's actually there
                peek_results = search_manager.search_client.search(
                    search_text="*",
                    select=["metadata_storage_name", "project", "metadata_storage_last_modified"],
                    top=20
                )
                peek_list = list(peek_results)
                if peek_list:
                    st.write(f"Index contains at least {len(peek_list)} documents. Here are the top 20:")
                    peek_data = []
                    for d in peek_list:
                        peek_data.append({
                            "Name": d.get('metadata_storage_name'),
                            "Project": d.get('project'),
                            "Modified": d.get('metadata_storage_last_modified')
                        })
                    st.table(peek_data)
                else:
                    st.error("âš ï¸ The Index appears to be COMPLETELY EMPTY.")
            except Exception as e:
                st.error(f"Failed to peek index: {e}")

        # 2. Blob Verification
        st.subheader("2. Blob Verification")
            path = first['metadata_storage_path']
            blob_path = None
            
            if "https://direct_fetch/" in path:
                st.warning("âš ï¸ Using 'direct_fetch' scheme. This is a virtual path.")
                blob_path = path.replace("https://direct_fetch/", "").split('#')[0]
            elif CONTAINER_NAME in path:
                try:
                    blob_path = path.split(f"/{CONTAINER_NAME}/")[1].split('#')[0]
                    blob_path = urllib.parse.unquote(blob_path)
                except:
                    pass
            
            if blob_path:
                st.write(f"**Extracted Blob Path:** `{blob_path}`")
                blob_client = container_client.get_blob_client(blob_path)
                if blob_client.exists():
                    st.success("âœ… Blob exists in storage.")
                else:
                    st.error("âŒ Blob DOES NOT exist at this path!")
                    
                    # Search for it
                    st.write("Searching for file in container...")
                    found_blobs = list(container_client.list_blobs(name_starts_with=os.path.dirname(blob_path)))
                    if found_blobs:
                        st.write("Found similar blobs:")
                        for b in found_blobs:
                            st.code(b.name)
                    else:
                        st.warning("No similar blobs found.")
            else:
                st.error("Could not extract blob path from metadata.")

            # 3. List Page Check
            st.subheader("3. List Page Check")
            list_keywords = ["PIPING AND INSTRUMENT DIAGRAM FOR LIST", "DRAWING LIST", "ë„ë©´ ëª©ë¡"]
            found_list = False
            
            for doc in results:
                # Handle None content safely
                content = doc.get('content')
                if content is None:
                    content = ""
                    st.warning(f"âš ï¸ Document '{doc['metadata_storage_name']}' has NO CONTENT (NULL).")
                
                content_upper = content.upper()
                if any(k in content_upper for k in list_keywords):
                    st.success(f"âœ… Found List Page! Name: `{doc['metadata_storage_name']}`")
                    st.text_area("Content Preview", content[:500], height=150)
                    found_list = True
                    break
            
            if not found_list:
                st.error("âŒ List Page NOT found in the top 50 results.")
                st.write("Top 5 Results Content Snippets:")
                for i, doc in enumerate(results[:5]):
                    content_preview = (doc.get('content') or "")[:100]
                    st.text(f"{i+1}. {doc['metadata_storage_name']}: {content_preview}...")

            # 4. Cleanup Tool
            st.divider()
            st.subheader("4. Index Cleanup")
            st.warning("If this document is corrupt (No Content / No Project), you can delete it here.")
            
            if st.button(f"ğŸ—‘ï¸ Delete ALL {len(results)} found documents from Index"):
                try:
                    # Collect IDs
                    ids_to_delete = [{"id": doc['id']} for doc in results]
                    search_manager.search_client.delete_documents(documents=ids_to_delete)
                    st.success(f"Successfully deleted {len(results)} documents.")
                    st.rerun()
                except Exception as e:
                    st.error(f"Delete failed: {e}")

        else:
            st.error("No documents found in index matching this filename.")

    # -----------------------------
    # ë””ë²„ê¹… ë„êµ¬ (Debug Tools)
    # -----------------------------
    if user_role == 'admin':
        with st.expander("ğŸ› ï¸ ì¸ë±ìŠ¤ ë° ê²€ìƒ‰ ì§„ë‹¨ (Debug Tools)", expanded=False):
            st.warning("ì´ ë„êµ¬ëŠ” ê²€ìƒ‰ ë¬¸ì œë¥¼ ì§„ë‹¨í•˜ê¸° ìœ„í•œ ê²ƒì…ë‹ˆë‹¤.")
            
            # Secret Inspector
            st.write("### ğŸ” ìê²© ì¦ëª… í™•ì¸ (Secret Inspector)")
            def mask_secret(s):
                if not s: return "Not Set"
                if len(s) <= 8: return "*" * len(s)
                return s[:4] + "*" * (len(s)-8) + s[-4:]
            
            secrets_to_check = {
                "AZURE_STORAGE_CONNECTION_STRING": STORAGE_CONN_STR,
                "AZURE_BLOB_CONTAINER_NAME": CONTAINER_NAME,
                "AZURE_OPENAI_ENDPOINT": AZURE_OPENAI_ENDPOINT,
                "AZURE_OPENAI_KEY": AZURE_OPENAI_KEY,
                "AZURE_SEARCH_ENDPOINT": SEARCH_ENDPOINT,
                "AZURE_SEARCH_KEY": SEARCH_KEY,
                "AZURE_TRANSLATOR_KEY": TRANSLATOR_KEY,
                "AZURE_DOC_INTEL_ENDPOINT": AZURE_DOC_INTEL_ENDPOINT,
                "AZURE_DOC_INTEL_KEY": AZURE_DOC_INTEL_KEY
            }
            
            import pandas as pd
            secret_data = []
            for k, v in secrets_to_check.items():
                secret_data.append({"Secret Key": k, "Status": "âœ… Loaded" if v else "âŒ Missing", "Value (Masked)": mask_secret(v)})
            
            st.table(pd.DataFrame(secret_data))
            
            st.write("---")
            
            if st.button("ğŸ” ì¸ë±ìŠ¤ ìƒíƒœ ë° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"):
                try:
                    search_manager = get_search_manager()
                    client = search_manager.search_client
                    
                    st.write("### 1. ì¸ë±ìŠ¤ ë¬¸ì„œ í™•ì¸ (project='drawings_analysis')")
                    results = client.search(search_text="*", filter="project eq 'drawings_analysis'", select=["id", "metadata_storage_name", "project"], top=20)
                    
                    docs = list(results)
                    st.write(f"Found {len(docs)} docs with project='drawings_analysis'")
                    
                    if docs:
                        for doc in docs:
                            st.code(f"ID: {doc['id']}\nName: {doc['metadata_storage_name']}\nProject: {doc['project']}")
                    
                    st.write("---")
                    st.write("### 1-B. ì¸ë±ìŠ¤ ë¬¸ì„œ í™•ì¸ (ì „ì²´ - í•„í„° ì—†ìŒ)")
                    results_all = client.search(search_text="*", select=["id", "metadata_storage_name", "project"], top=20)
                    docs_all = list(results_all)
                    st.write(f"Found {len(docs_all)} docs in total (top 20)")
                    for doc in docs_all:
                        proj = doc.get('project', 'None')
                        st.code(f"Name: {doc['metadata_storage_name']}\nProject: {proj}")
                    
                    st.write("---")
                    st.write("### 2. í‚¤ì›Œë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ('foundation loading data')")
                    search_results = client.search(search_text="foundation loading data", filter="project eq 'drawings_analysis'", top=5, select=["metadata_storage_name", "content"])
                    search_docs = list(search_results)
                    
                    st.write(f"ê²€ìƒ‰ ê²°ê³¼: {len(search_docs)}ê°œ")
                    for doc in search_docs:
                        st.text(f"Match: {doc['metadata_storage_name']}")
                        st.caption(f"Content: {doc['content'][:200]}...")
                    
                    st.write("---")
                    st.write("### 3. ì™€ì¼ë“œì¹´ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ('*')")
                    wild_results = client.search(search_text="*", filter="project eq 'drawings_analysis'", top=5, select=["metadata_storage_name", "content"])
                    wild_docs = list(wild_results)
                    
                    st.write(f"ê²€ìƒ‰ ê²°ê³¼: {len(wild_docs)}ê°œ")
                    for doc in wild_docs:
                        st.text(f"Match: {doc['metadata_storage_name']}")
                        st.caption(f"Content: {doc['content'][:200]}...")
                        
                except Exception as e:
                    st.error(f"ì§„ë‹¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    st.code(str(e))
            
            st.write("---")
            st.write("### ğŸ” ì¸ë±ìŠ¤ ë°ì´í„° í™•ì¸")
            if st.button("ğŸ“‘ ì¸ë±ìŠ¤ëœ ëª¨ë“  íŒŒì¼ëª… ë³´ê¸°"):
                with st.spinner("ì¸ë±ìŠ¤ ì¡°íšŒ ì¤‘..."):
                    try:
                        search_manager = get_search_manager()
                        # Get all docs (limit to top 1000 to be safe)
                        results = search_manager.search("*", select=["metadata_storage_name"], top=1000)
                        indexed_files = set()
                        for res in results:
                            # Remove page suffix (p.N) to get base filename
                            name = res['metadata_storage_name']
                            base_name = name.split(' (p.')[0]
                            indexed_files.add(base_name)
                        
                        st.write(f"ì´ {len(indexed_files)}ê°œì˜ íŒŒì¼ì´ ì¸ë±ìŠ¤ì—ì„œ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        st.dataframe(list(indexed_files), use_container_width=True)
                    except Exception as e:
                        st.error(f"ì¡°íšŒ ì‹¤íŒ¨: {e}")

            st.write("---")
            st.write("### ğŸ§ª ì‚¬ìš©ì ì§€ì • ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
            debug_query = st.text_input("ê²€ìƒ‰ì–´ ì…ë ¥ (ì˜ˆ: filter element)", key="debug_query")
            if st.button("ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤í–‰", key="run_debug_search"):
                if debug_query:
                    try:
                        search_manager = get_search_manager()
                        client = search_manager.search_client
                        
                        st.write(f"Query: '{debug_query}'")
                        # Use 'any' search mode to match behavior
                        results = client.search(
                            search_text=debug_query, 
                            filter="project eq 'drawings_analysis'", 
                            search_mode="any",
                            select=["metadata_storage_name", "content"],
                            top=10
                        )
                        docs = list(results)
                        st.write(f"ê²€ìƒ‰ ê²°ê³¼: {len(docs)}ê°œ")
                        
                        if docs:
                            for doc in docs:
                                st.text(f"Match: {doc['metadata_storage_name']}")
                                st.caption(f"Content: {doc['content'][:200]}...")
                        else:
                            st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    except Exception as e:
                        st.error(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

            st.write("---")
            st.write("### âš ï¸ ì¸ë±ìŠ¤ ì´ˆê¸°í™”")
            if st.button("ğŸ—‘ï¸ ëª¨ë“  ë„ë©´ ë°ì´í„° ì‚­ì œ (Index & Blob)", type="primary"):
                try:
                    # 1. Delete all blobs in any drawings/, json/ folder (Global reset)
                    blob_service_client = get_blob_service_client()
                    container_client = blob_service_client.get_container_client(CONTAINER_NAME)
                    
                    # List all blobs and filter for drawings or json
                    blobs = container_client.list_blobs()
                    deleted_blobs = 0
                    for blob in blobs:
                        if '/drawings/' in blob.name or blob.name.startswith('drawings/') or '/json/' in blob.name or blob.name.startswith('json/'):
                            container_client.delete_blob(blob.name)
                            deleted_blobs += 1
                    
                    # 2. Delete all docs in index with project='drawings_analysis'
                    search_manager = get_search_manager()
                    
                    deleted_total = 0
                    while True:
                        results = search_manager.search_client.search(
                            search_text="*",
                            filter="project eq 'drawings_analysis'",
                            select=["id"],
                            top=1000
                        )
                        ids_to_delete = [{"id": doc['id']} for doc in results]
                        if not ids_to_delete:
                            break
                            
                        search_manager.search_client.delete_documents(documents=ids_to_delete)
                        deleted_total += len(ids_to_delete)
                        if len(ids_to_delete) < 1000:
                            break
                    
                    st.success(f"ëª¨ë“  ë„ë©´ ë°ì´í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤. (Blob ì‚­ì œ ì™„ë£Œ, Index {deleted_total}ê°œ ì‚­ì œ ì™„ë£Œ) ì´ì œ íŒŒì¼ì„ ë‹¤ì‹œ ì—…ë¡œë“œí•˜ì„¸ìš”.")
                    st.rerun()
                except Exception as e:
                    st.error(f"ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

            if st.button("ğŸ§¹ 'í˜ì´ì§€ ë²ˆí˜¸ ì—†ëŠ”' ì¤‘ë³µ ë°ì´í„° ì •ë¦¬ (ê¶Œì¥)", help="ì¸ë±ìŠ¤ì—ì„œ (p.N) í˜•ì‹ì´ ì•„ë‹Œ ì˜ëª»ëœ ë°ì´í„°ë¥¼ ì°¾ì•„ ì‚­ì œí•©ë‹ˆë‹¤."):
                try:
                    search_manager = get_search_manager()
                    results = search_manager.search_client.search(
                        search_text="*",
                        filter="project eq 'drawings_analysis'",
                        select=["id", "metadata_storage_name"],
                        top=1000
                    )
                    
                    ids_to_delete = []
                    count = 0
                    for doc in results:
                        name = doc['metadata_storage_name']
                        # Delete if it doesn't contain "(p." (standard page suffix)
                        if "(p." not in name:
                            ids_to_delete.append({"id": doc['id']})
                            count += 1
                    
                    if ids_to_delete:
                        search_manager.search_client.delete_documents(documents=ids_to_delete)
                        st.success(f"ì •ë¦¬ ì™„ë£Œ! {count}ê°œì˜ ì¤‘ë³µ/ì˜ëª»ëœ ë¬¸ì„œë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
                        st.rerun()
                    else:
                        st.info("ì‚­ì œí•  ì˜ëª»ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¸ë±ìŠ¤ê°€ ê¹¨ë—í•©ë‹ˆë‹¤! âœ¨")
                        
                except Exception as e:
                    st.error(f"ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

            if st.button("ğŸ·ï¸ ëˆ„ë½ëœ 'drawings_analysis' íƒœê·¸ ë³µêµ¬", help="ë“œë¡œì‰ í´ë”ì— ìˆì§€ë§Œ í”„ë¡œì íŠ¸ íƒœê·¸ê°€ ì—†ëŠ” ë¬¸ì„œë¥¼ ì°¾ì•„ íƒœê·¸ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."):
                try:
                    search_manager = get_search_manager()
                    # Search for all docs with missing project tag and filter path in Python
                    results = search_manager.search_client.search(
                        search_text="*",
                        filter="(project eq null)",
                        select=["id", "metadata_storage_name", "metadata_storage_path", "content", "content_exact", "metadata_storage_last_modified", "metadata_storage_size", "metadata_storage_content_type"],
                        top=10000 # Increase to cover all docs
                    )
                    
                    docs_to_fix = []
                    for doc in results:
                        # Filter by path in Python
                        if '/drawings/' in doc.get('metadata_storage_path', ''):
                            doc['project'] = 'drawings_analysis'
                            docs_to_fix.append(doc)
                    
                    if docs_to_fix:
                        success, msg = search_manager.upload_documents(docs_to_fix)
                        if success:
                            st.success(f"ë³µêµ¬ ì™„ë£Œ! {len(docs_to_fix)}ê°œì˜ ë¬¸ì„œì— 'drawings_analysis' íƒœê·¸ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
                            st.rerun()
                        else:
                            st.error(f"ë³µêµ¬ ì‹¤íŒ¨: {msg}")
                    else:
                        st.info("íƒœê·¸ë¥¼ ë³µêµ¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    st.error(f"íƒœê·¸ ë³µêµ¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

            if st.button("ğŸ“Š ì¸ë±ìŠ¤ í†µê³„ í™•ì¸", help="í”„ë¡œì íŠ¸ë³„ ë¬¸ì„œ ê°œìˆ˜ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."):
                try:
                    search_manager = get_search_manager()
                    
                    # Count drawings_analysis
                    drawings_res = search_manager.search_client.search(
                        search_text="*",
                        filter="project eq 'drawings_analysis'",
                        include_total_count=True,
                        top=0
                    )
                    drawings_count = drawings_res.get_count()
                    
                    # Count others (likely standard indexed)
                    others_res = search_manager.search_client.search(
                        search_text="*",
                        filter="project eq null",
                        include_total_count=True,
                        top=0
                    )
                    others_count = others_res.get_count()
                    
                    st.write(f"**ë„ë©´ ë¶„ì„ ë°ì´í„° (drawings_analysis):** {drawings_count}ê°œ")
                    st.write(f"**ì¼ë°˜ ë¬¸ì„œ ë°ì´í„° (Standard Indexer):** {others_count}ê°œ")
                    
                    # Check Standard Indexer Status
                    st.divider()
                    st.write("**í‘œì¤€ ì¸ë±ì„œ (Standard Indexer) ìƒíƒœ í™•ì¸:**")
                    # Try common indexer names
                    for idx_name in ["pdf-indexer", "indexer-all", "indexer-drawings"]:
                        try:
                            status = search_manager.indexer_client.get_indexer_status(idx_name)
                            last_res = status.last_result
                            if last_res:
                                st.write(f"- `{idx_name}`: {last_res.status} (ì„±ê³µ: {last_res.item_count}, ì‹¤íŒ¨: {last_res.failed_item_count})")
                                if last_res.failed_item_count > 0:
                                    with st.expander(f"âŒ {idx_name} ì—ëŸ¬ ìƒì„¸ ë³´ê¸°"):
                                        for err in last_res.errors[:5]:
                                            st.error(f"ë¬¸ì„œ: {err.key}\nì—ëŸ¬: {err.message}")
                            else:
                                st.write(f"- `{idx_name}`: ì‹¤í–‰ ê¸°ë¡ ì—†ìŒ")
                        except:
                            pass

                    if drawings_count == 0 and others_count > 0:
                        st.warning("ë„ë©´ ë°ì´í„°ê°€ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤. ì¸ë±ì‹± ê³¼ì •ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    st.error(f"í†µê³„ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

            with st.expander("ğŸ” ì¸ë±ìŠ¤ ìƒì„¸ ì§„ë‹¨ ë„êµ¬", expanded=False):
                st.caption("ì¸ë±ìŠ¤ì— ì €ì¥ëœ ì‹¤ì œ íŒŒì¼ëª…ê³¼ íƒœê·¸ë¥¼ ì§ì ‘ í™•ì¸í•©ë‹ˆë‹¤.")
                
                # Add search input for specific file diagnosis (Outside button for persistence)
                diag_query = st.text_input("ì§„ë‹¨í•  íŒŒì¼ëª… ê²€ìƒ‰ (ì¼ë¶€ë§Œ ì…ë ¥ ê°€ëŠ¥)", value="", key="diag_query")
                diag_path_filter = st.checkbox("'/drawings/' ê²½ë¡œë§Œ ë³´ê¸°", value=True, key="diag_path_filter")
                
                if st.button("ğŸ“‹ ì§„ë‹¨ ì‹¤í–‰ (ìµœê·¼ 100ê°œ)"):
                    try:
                        search_manager = get_search_manager()
                        
                        # Use a more inclusive search for diagnosis
                        # If query is provided, use it as search_text. If not, use *
                        results = search_manager.search_client.search(
                            search_text=diag_query if diag_query else "*",
                            select=["metadata_storage_name", "project", "metadata_storage_path"],
                            top=1000 # Increase for better diagnosis
                        )
                        
                        dump_data = []
                        for doc in results:
                            name = doc.get('metadata_storage_name', '')
                            path = doc.get('metadata_storage_path', '')
                            
                            if diag_path_filter and '/drawings/' not in path:
                                continue
                                
                            dump_data.append({
                                "Name": name,
                                "Project": doc.get('project'),
                                "Path": path
                            })
                        
                        if dump_data:
                            st.write(f"ê²€ìƒ‰ ê²°ê³¼: {len(dump_data)}ê°œì˜ ë¬¸ì„œ ë°œê²¬")
                            st.table(dump_data)
                        else:
                            st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ëª…ì´ ì¸ë±ìŠ¤ì— ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ í•„í„°ì— ê±¸ëŸ¬ì¡Œì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                            
                        # Extra check: Search by path only if query failed
                        if diag_query and not dump_data:
                            st.info(f"'{diag_query}'ë¡œ ê²€ìƒ‰ëœ ê²°ê³¼ê°€ ì—†ì–´ ê²½ë¡œ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì‹œ ì°¾ìŠµë‹ˆë‹¤...")
                            # Use startswith on metadata_storage_path (SimpleField/Filterable)
                            # We don't know the full prefix, but we can try to find anything in drawings
                            path_results = search_manager.search_client.search(
                                search_text="*",
                                filter="startswith(metadata_storage_path, 'https://')", # Broad filter
                                select=["metadata_storage_name", "project", "metadata_storage_path"],
                                top=5000 # Increase to cover more docs
                            )
                            # Filter for '/drawings/' in Python for maximum reliability
                            path_data = [
                                {"Name": d['metadata_storage_name'], "Project": d['project'], "Path": d['metadata_storage_path']} 
                                for d in path_results 
                                if '/drawings/' in d.get('metadata_storage_path', '')
                            ]
                            if path_data:
                                st.write("'/drawings/' ê²½ë¡œì—ì„œ ë°œê²¬ëœ íŒŒì¼ë“¤ (ìµœê·¼ 100ê°œ ì¤‘):")
                                st.table(path_data[:20])
                            else:
                                st.error("'/drawings/' ê²½ë¡œì—ì„œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¸ë±ì„œê°€ í•´ë‹¹ í´ë”ë¥¼ ìŠ¤ìº”í•˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                                
                    except Exception as e:
                        st.error(f"ì§„ë‹¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            with st.expander("ğŸ“Š ì„ íƒëœ íŒŒì¼ í† í° ë¶„ì„ (Token Analyzer)", expanded=False):
                st.caption("íŠ¹ì • íŒŒì¼ì˜ ì¸ë±ìŠ¤ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ í† í° ì‚¬ìš©ëŸ‰ì„ í™•ì¸í•©ë‹ˆë‹¤.")
                target_file_input = st.text_input("ë¶„ì„í•  íŒŒì¼ëª… (ì¼ë¶€ë§Œ ì…ë ¥í•´ë„ ë¨)", value="PH20-810-EC115-00540")
                
                if st.button("ë¶„ì„ ì‹¤í–‰", key="analyze_token_btn"):
                    try:
                        search_manager = get_search_manager()
                        # Search for chunks matching the filename
                        results = search_manager.search_client.search(
                            search_text="*",
                            filter=f"search.ismatch('{target_file_input}', 'metadata_storage_name')",
                            select=["metadata_storage_name", "content", "metadata_storage_path"]
                        )
                        
                        results_list = list(results)
                        st.info(f"ê²€ìƒ‰ëœ ì²­í¬(Chunk) ìˆ˜: {len(results_list)}ê°œ")
                        
                        total_chars = 0
                        for i, doc in enumerate(results_list):
                            content = doc.get('content', '')
                            char_count = len(content)
                            total_chars += char_count
                            
                            with st.expander(f"Chunk {i+1}: {doc.get('metadata_storage_name')} ({char_count}ì)"):
                                st.code(content[:1000] + ("..." if len(content) > 1000 else ""))
                        
                        st.divider()
                        st.metric("ì´ ê¸€ì ìˆ˜ (Total Characters)", f"{total_chars:,}")
                        est_tokens = int(total_chars / 4)
                        st.metric("ì˜ˆìƒ í† í° ìˆ˜ (Estimated Tokens)", f"{est_tokens:,}")
                        
                        if est_tokens > 5000:
                            st.warning(f"âš ï¸ í† í° ìˆ˜ê°€ ë§ìŠµë‹ˆë‹¤ ({est_tokens} > 5000). AI ë‹µë³€ ìƒì„± ì‹œ 'Token Limit Exceeded' ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                        else:
                            st.success(f"âœ… í† í° ìˆ˜ê°€ ì ì ˆí•©ë‹ˆë‹¤ ({est_tokens}).")
                            
                    except Exception as e:
                        st.error(f"ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”", key="clear_rag_chat"):
                st.session_state.rag_chat_messages = []
                st.rerun()

    st.markdown("---")

if menu == "ì—‘ì…€ë°ì´í„° ìë™ì¶”ì¶œ":
    # Integrated Excel Tool
    excel_manager.render_excel_tool()

if menu == "ì‚¬ì§„ëŒ€ì§€ ìë™ì‘ì„±":
    st.caption("ê±´ì„¤ í˜„ì¥ ì‚¬ì§„ì„ ì—…ë¡œë“œí•˜ì—¬ Excel ì‚¬ì§„ëŒ€ì§€ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
    
    # Embed Photo Log app via iframe
    st.components.v1.iframe(
        src="https://photo-log-a0215.web.app/",
        height=800,
        scrolling=True
    )

if menu == "ì‘ì—…ê³„íš ë° íˆ¬ì…ë¹„ ìë™ì‘ì„±":
    st.caption("ì‘ì—… ê³„íšì„ ìˆ˜ë¦½í•˜ê³  íˆ¬ì…ë¹„ë¥¼ ìë™ìœ¼ë¡œ ì‚°ì¶œí•©ë‹ˆë‹¤.")
    
    # Embed Work Schedule app via iframe
    st.components.v1.iframe(
        src="https://workschedule-7b1cf.web.app/",
        height=800,
        scrolling=True
    )

if menu == "ê´€ë¦¬ì ì„¤ì •":
    # st.subheader("âš™ï¸ ê´€ë¦¬ì ì„¤ì •") - Removed to avoid duplication
    st.info("Azure AI Search ë¦¬ì†ŒìŠ¤ë¥¼ ì´ˆê¸°í™”í•˜ê±°ë‚˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.")
    
    # ì¸ë±ì‹± ëŒ€ìƒ í´ë” ì„¤ì •
    # í´ë” ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    folder_options = ["(ì „ì²´)"]
    try:
        blob_service_client = get_blob_service_client()
        container_client = blob_service_client.get_container_client(CONTAINER_NAME)
        # walk_blobsë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœìƒìœ„ í´ë”ë§Œ ì¡°íšŒ
        for blob in container_client.walk_blobs(delimiter='/'):
            if blob.name.endswith('/'):
                folder_options.append(blob.name.strip('/'))
    except Exception as e:
        st.warning(f"í´ë” ëª©ë¡ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}")
        folder_options.append("GULFLNG") # Fallback

    # ê¸°ë³¸ê°’ ì„¤ì • (GULFLNGê°€ ìˆìœ¼ë©´ ê·¸ê±¸ë¡œ, ì—†ìœ¼ë©´ ì „ì²´)
    default_idx = 0
    if "GULFLNG" in folder_options:
        default_idx = folder_options.index("GULFLNG")

    selected_folder = st.selectbox(
        "ì¸ë±ì‹± ëŒ€ìƒ í´ë” ì„ íƒ", 
        folder_options, 
        index=default_idx,
        help="ì¸ë±ì‹±í•  í”„ë¡œì íŠ¸ í´ë”ë¥¼ ì„ íƒí•˜ì„¸ìš”."
    )
    
    
    # '(ì „ì²´)' ì„ íƒ ì‹œ Noneìœ¼ë¡œ ì²˜ë¦¬
    target_folder = None if selected_folder == "(ì „ì²´)" else selected_folder
    
    st.info("ğŸ’¡ **í´ë”ë³„ ì¸ë±ì‹±**: ê° í´ë”ëŠ” ë…ë¦½ì ìœ¼ë¡œ ì¸ë±ì‹±ë©ë‹ˆë‹¤. ë‹¤ë¥¸ í´ë”ì˜ ë°ì´í„°ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    confirm_reset = st.checkbox("ìœ„ í´ë”ë¥¼ ì¸ë±ì‹± ëŒ€ìƒìœ¼ë¡œ ì„¤ì •í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.", key="confirm_reset")
    
    if st.button("ğŸš€ í´ë” ì¸ë±ì‹± ì„¤ì • (Data Source, Indexer)", disabled=not confirm_reset):
        with st.spinner("ë¦¬ì†ŒìŠ¤ ìƒì„± ì¤‘..."):
            manager = get_search_manager()
            
            # 1. Index í™•ì¸/ìƒì„± (í•œë²ˆë§Œ í•„ìš”)
            st.write("1. Index í™•ì¸ ì¤‘...")
            success, msg = manager.create_index()
            if success:
                st.success(msg)
            else:
                st.error(msg)
                
            # 2. Data Source (í´ë”ë³„)
            st.write(f"2. Data Source ìƒì„± ì¤‘... (í´ë”: {selected_folder})")
            success, msg, datasource_name = manager.create_data_source(
                SEARCH_DATASOURCE_NAME, 
                STORAGE_CONN_STR, 
                CONTAINER_NAME, 
                query=target_folder,
                folder_name=target_folder
            )
            if success:
                st.success(msg)
            else:
                st.error(msg)
                st.stop()  # Stop execution if datasource creation fails
                
            # 2.5 Skillset (OCR) - Optional
            skillset_name = None
            enable_ocr = st.checkbox("ğŸ“¸ OCR(ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ) í™œì„±í™”", value=False, help="PDF ë„ë©´ì´ë‚˜ ì´ë¯¸ì§€ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. Azure AI Services í‚¤ê°€ í•„ìš”í•˜ë©° ë¹„ìš©ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            if enable_ocr:
                st.write(f"2.5. Skillset (OCR) ìƒì„± ì¤‘...")
                # Use Translator Key as Cognitive Services Key (assuming it's a multi-service key)
                cog_key = st.secrets.get("AZURE_TRANSLATOR_KEY", os.environ.get("AZURE_TRANSLATOR_KEY"))
                
                if not cog_key:
                    st.warning("âš ï¸ Azure AI Services í‚¤(AZURE_TRANSLATOR_KEY)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ OCRì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                else:
                    skillset_name = f"skillset-{target_folder}" if target_folder else "skillset-all"
                    success, msg = manager.create_skillset(skillset_name, cog_key)
                    if success:
                        st.success(msg)
                    else:
                        st.error(f"Skillset ìƒì„± ì‹¤íŒ¨: {msg}")
                        skillset_name = None # Fallback to no skillset
                
            # 3. Indexer (í´ë”ë³„)
            st.write(f"3. Indexer ìƒì„± ì¤‘... (í´ë”: {selected_folder})")
            # ê¸°ì¡´ ì¸ë±ì„œ ì‚­ì œ (ê°™ì€ í´ë”ì˜ ì´ì „ ì„¤ì • ì œê±°)
            manager.delete_indexer(target_folder)
            success, msg, indexer_name = manager.create_indexer(target_folder, datasource_name, skillset_name=skillset_name)
            if success:
                st.success(msg)
                st.info(f"âœ… '{selected_folder}' í´ë”ì— ëŒ€í•œ ì¸ë±ì‹± ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ 'ì¸ë±ì„œ ìˆ˜ë™ ì‹¤í–‰'ì„ ëˆŒëŸ¬ ì¸ë±ì‹±ì„ ì‹œì‘í•˜ì„¸ìš”.")
            else:
                st.error(msg)
    
    st.divider()
    
    # ------------------------------------------------------------------
    # 4. ì¸ë±ìŠ¤ ë‚´ìš© ì¡°íšŒ (ë””ë²„ê¹…ìš©)
    # ------------------------------------------------------------------
    st.subheader("ğŸ” ì¸ë±ìŠ¤ ë‚´ìš© ì¡°íšŒ (OCR í™•ì¸ìš©)")
    with st.expander("íŠ¹ì • íŒŒì¼ì˜ ì¸ë±ì‹±ëœ ë‚´ìš© í™•ì¸í•˜ê¸°"):
        target_filename = st.text_input("í™•ì¸í•  íŒŒì¼ëª… (ì˜ˆ: drawing.pdf)", help="ì •í™•í•œ íŒŒì¼ëª…ì„ ì…ë ¥í•˜ì„¸ìš”.")
        if st.button("ë‚´ìš© ì¡°íšŒ"):
            if target_filename:
                manager = get_search_manager()
                with st.spinner("ì¡°íšŒ ì¤‘..."):
                    content = manager.get_document_content(target_filename)
                    st.text_area("ì¸ë±ì‹±ëœ ë‚´ìš© (ì•ë¶€ë¶„ 2000ì)", content[:2000], height=300)
            else:
                st.warning("íŒŒì¼ëª…ì„ ì…ë ¥í•˜ì„¸ìš”.")

    st.divider()
    
    # ìˆ˜ë™ ì‹¤í–‰ ì•ˆë‚´ ë° í™•ì¸
    st.info(f"ğŸ“‚ **í˜„ì¬ ì„ íƒëœ í´ë”**: {selected_folder}")
    st.markdown("ìˆ˜ë™ ì¸ë±ì„œ ì‹¤í–‰ì€ ì„ íƒí•œ í´ë”ì˜ ìƒˆ íŒŒì¼ ë˜ëŠ” ë³€ê²½ëœ íŒŒì¼ì„ ê²€ìƒ‰ ì—”ì§„ì— ë°˜ì˜í•©ë‹ˆë‹¤.")
    
    confirm_run = st.checkbox("ìœ„ í´ë”ë¥¼ ì¸ë±ì‹±í•˜ëŠ” ê²ƒì„ í™•ì¸í–ˆìœ¼ë©°, ì§„í–‰í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.", key="confirm_run")
    
    if st.button("â–¶ï¸ ì¸ë±ì„œ ìˆ˜ë™ ì‹¤í–‰", disabled=not confirm_run):
        manager = get_search_manager()
        success, msg = manager.run_indexer(target_folder)
        if success:
            st.success(msg)
            st.info("ì¸ë±ì‹±ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ 'ìƒíƒœ í™•ì¸' ë²„íŠ¼ì„ ëˆŒëŸ¬ ì§„í–‰ ìƒí™©ì„ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”.")
        else:
            st.error(msg)
            
    # Add Delete Indexer Button
    if st.button("ğŸ›‘ ì¸ë±ì„œ ì‚­ì œ (ìë™ ì¸ë±ì‹± ì¤‘ì§€)", help="ìë™ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ” ì¸ë±ì„œë¥¼ ì‚­ì œí•˜ì—¬ ì¤‘ë³µ ì¸ë±ì‹±ì„ ë°©ì§€í•©ë‹ˆë‹¤."):
        manager = get_search_manager()
        indexer_name = f"indexer-{target_folder}" if target_folder else "indexer-all"
        try:
            manager.indexer_client.delete_indexer(indexer_name)
            st.success(f"ì¸ë±ì„œ '{indexer_name}'ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ìë™ ì¸ë±ì‹±ì´ ì¤‘ì§€ë©ë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"ì¸ë±ì„œ ì‚­ì œ ì‹¤íŒ¨: {e}")
            
    st.divider()
    
    col_status, col_refresh = st.columns([3, 1])
    with col_status:
        st.markdown("### ğŸ“Š ì¸ë±ì‹± í˜„í™© ëª¨ë‹ˆí„°ë§")
    with col_refresh:
        auto_refresh = st.checkbox("ìë™ ìƒˆë¡œê³ ì¹¨ (5ì´ˆ)", value=False)

    # ìƒíƒœ í™•ì¸ ë¡œì§ (ë²„íŠ¼ í´ë¦­ ë˜ëŠ” ìë™ ìƒˆë¡œê³ ì¹¨)
    if st.button("ìƒíƒœ ë° ì§„í–‰ë¥  í™•ì¸") or auto_refresh:
        manager = get_search_manager()
        
        # 1. ì†ŒìŠ¤ íŒŒì¼ ê°œìˆ˜ í™•ì¸ (ì§„í–‰ë¥  ê³„ì‚°ìš©)
        with st.spinner("ì†ŒìŠ¤ íŒŒì¼ ê°œìˆ˜ ê³„ì‚° ì¤‘..."):
            total_blobs = manager.get_source_blob_count(STORAGE_CONN_STR, CONTAINER_NAME, folder_path=target_folder)
        
        # 2. ì¸ë±ì„œ ìƒíƒœ í™•ì¸
        status_info = manager.get_indexer_status(target_folder)
        
        # ìƒíƒœ ì–¸íŒ©
        status = status_info.get("status")
        item_count = status_info.get("item_count", 0)
        failed_count = status_info.get("failed_item_count", 0)
        error_msg = status_info.get("error_message")
        errors = status_info.get("errors", [])
        warnings = status_info.get("warnings", [])
        
        # 3. ì¸ë±ìŠ¤ ë¬¸ì„œ ê°œìˆ˜
        doc_count = manager.get_document_count()
        
        # UI í‘œì‹œ
        st.metric(label="ì´ ì†ŒìŠ¤ íŒŒì¼ ìˆ˜", value=f"{total_blobs}ê°œ")
        
        # ì§„í–‰ë¥  ê³„ì‚° (ì‹¤ì œ ì¸ë±ìŠ¤ëœ ë¬¸ì„œ ìˆ˜ ê¸°ì¤€)
        if total_blobs > 0:
            progress = min(doc_count / total_blobs, 1.0)
        else:
            progress = 0.0
            
        st.progress(progress, text=f"ì¸ë±ì‹± ì§„í–‰ë¥ : {int(progress * 100)}% ({doc_count}/{total_blobs})")
        
        # ìƒíƒœ ë©”ì‹œì§€
        if status == "inProgress":
            st.info(f"â³ ì¸ë±ì‹± ì§„í–‰ ì¤‘... (ì²˜ë¦¬ëœ ë¬¸ì„œ: {item_count}, ì‹¤íŒ¨: {failed_count})")
            if auto_refresh:
                time.sleep(5)
                st.rerun()
        elif status == "success":
            st.success(f"âœ… ì¸ë±ì‹± ì™„ë£Œ! (ì´ ì¸ë±ìŠ¤ ë¬¸ì„œ: {doc_count}ê°œ)")
        elif status == "error":
            st.error(f"âŒ ì¸ë±ì‹± ì˜¤ë¥˜ ë°œìƒ: {error_msg}")
        elif status == "transientFailure":
            st.warning("âš ï¸ ì¼ì‹œì  ì˜¤ë¥˜ ë°œìƒ (ì¬ì‹œë„ ì¤‘...)")
        else:
            st.write(f"ìƒíƒœ: {status}")

        # ì˜¤ë¥˜ ìƒì„¸ í‘œì‹œ
        if failed_count > 0 or errors:
            st.error(f"âŒ ì‹¤íŒ¨í•œ ë¬¸ì„œ: {failed_count}ê°œ")
            with st.expander("ğŸš¨ ì˜¤ë¥˜ ìƒì„¸ ë¡œê·¸ í™•ì¸", expanded=True):
                for err in errors:
                    st.write(f"- {err}")
        
        if warnings:
            with st.expander("âš ï¸ ê²½ê³  ë¡œê·¸ í™•ì¸"):
                for warn in warnings:
                    st.warning(f"- {warn}")

if menu == "ì‚¬ìš©ì ì„¤ì •":
    from modules.user_settings_module import render_user_settings
    render_user_settings(auth_manager)


